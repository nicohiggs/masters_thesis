\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Comparison of county parameter estimates between traditional regression (no pooling) and multilevel regression (partial pooling). Notice how the partial pooling estimates ``shrink toward the mean''. Further note how this shrinkage is greater for the counties with fewer observations and lesser for counties with more observations.\relax }}{10}{figure.caption.13}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Under ideal circumstances a Markov chain will first converge to the typical set (a) and then explore it efficiently (b). Unfortunately, in higher dimensions most MCMC algorithms struggle to explore the typical set and inefficiently sample a small portion (c, green). We desire algorithms that make use of the geometry of the target distribution to properly explore the typical set during sampling (d). Images are from Figures 7, 10, 11 of \cite {Betancourt2017}. Permission to use was granted by the author under a CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/).\relax }}{14}{figure.caption.14}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{14}{subfigure.2.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{14}{subfigure.2.2}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {}}}{14}{subfigure.2.3}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {}}}{14}{subfigure.2.4}
\contentsline {figure}{\numberline {3.3}{\ignorespaces The gradient and corresponding vector field of a probability distribution points to its mode which is often away from the typical set in higher dimensions (a). Ideally we want to twist the vector field to align with the typical set (b). The mode, gradient, and typical set of a probabilistic system are mathematically equivalent to a planet, gravitational field, and orbit in a physical system (c). Adding momentum to the system to cause a satellite to enter a stable orbit (d) is equivalent to twisting a vector field to align with the typical set of a probabilistic system. Images are from Figures 12, 13, 14, 17 of \cite {Betancourt2017}. Permission to use was granted by the author under a CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/).\relax }}{17}{figure.caption.15}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{17}{subfigure.3.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{17}{subfigure.3.2}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {}}}{17}{subfigure.3.3}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {}}}{17}{subfigure.3.4}
\contentsline {figure}{\numberline {3.4}{\ignorespaces Example of how increasing model complexity leads to better model fit on the train-set, but can come at the cost of increasingly worse performance on the test-set. Model fit here is measured visually and in terms of mean-squared-error (MSE: lower is better) and R-squared ($R^2$: closer to 1 is better). The dataset in (a) is generated by a degree-2 polynomial with some added noise and is split into train and test sets. A degree-1 polynomial underfits the data (b). More complex polynomials improve the fit on the train-set (c, d, e). However, increasingly complex polynomials become overfit as evidenced by increasingly worse test-set performance (d, e). The overall trend of increasing model complexity, how it relates to underfitting and overfitting, and where the tradeoff is optimal is captured in (f).\relax }}{20}{figure.caption.16}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{20}{subfigure.4.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{20}{subfigure.4.2}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {}}}{20}{subfigure.4.3}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {}}}{20}{subfigure.4.4}
\contentsline {subfigure}{\numberline {(e)}{\ignorespaces {}}}{20}{subfigure.4.5}
\contentsline {subfigure}{\numberline {(f)}{\ignorespaces {}}}{20}{subfigure.4.6}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces An example of how a Bayesian model would be defined in an academic textbook or paper (a) and how a probabilistic programming language such as PyMC3 would create the model in Python code (b). Note how the priors have to be defined first because the code will be executed procedurally. The two definitions are essentially identical otherwise.\relax }}{31}{figure.caption.17}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{31}{subfigure.1.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{31}{subfigure.1.2}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces Comparison of models via their Log-Score (higher is better) on train and test sets, as well as the PSIS-LOO estimated Log-Score, for each league. The complete-pooling model under-fits, the no-pooling model over-fits, and the partial-pooling model provides the best trade-off in fitting the data while protecting against over-fitting. The PSIS-LOO estimates consistently predict how the models would rank on an unseen test-set.\relax }}{38}{figure.caption.18}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {NHL}}}{38}{subfigure.1.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {NBA}}}{38}{subfigure.1.2}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {MLB}}}{38}{subfigure.1.3}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {NFL}}}{38}{subfigure.1.4}
\contentsline {figure}{\numberline {6.2}{\ignorespaces Comparison of models via their PSIS-LOO estimated Log-Score for each league, ranked from best (highest) to worst (lowest) on the y-axis. The black points and lines represent the point estimate and its standard error. The grey triangle and lines represent the estimated difference and the standard error of the difference for each model relative to the best model. The standard error of the difference is generally much smaller than the standard error of the estimate because errors in the estimates for each model are highly correlated.\relax }}{39}{figure.caption.19}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {NHL}}}{39}{subfigure.2.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {NBA}}}{39}{subfigure.2.2}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {MLB}}}{39}{subfigure.2.3}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {NFL}}}{39}{subfigure.2.4}
\contentsline {figure}{\numberline {6.3}{\ignorespaces Comparison of distribution of home points in the models and the observed data for each league. The Negative Binomial model noticeably provides a better overall fit across each league.\relax }}{41}{figure.caption.20}
\contentsline {figure}{\numberline {6.4}{\ignorespaces Distributions of the estimated home advantage for the NHL, NBA, MLB, and NFL for pre and post COVID adjusted seasons. Home advantage for playoffs are reported for NHL and NBA because that is when their COVID restricted games took place. Home advantage for regular season is reported for MLB and NFL as their respective playoff seasons are too small for stable results. Red distributions represent COVID-19 bubble adjusted seasons.\relax }}{43}{figure.caption.22}
\contentsline {figure}{\numberline {6.5}{\ignorespaces Distributions of the estimated home advantage for the NHL, NBA, MLB, and NFL over the past 5 seasons from 2016-2020. Home advantage for playoffs are reported for NHL and NBA because that is when their COVID restricted games took place. Home advantage for regular season is reported for MLB and NFL as their respective playoff seasons are too small for stable results. Red distributions represent COVID-19 bubble adjusted seasons.\relax }}{45}{figure.caption.23}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {A.1}{\ignorespaces Offensive and Defensive team ratings for the 2020 NHL season. The points with the team labels next to them are ratings generated by traditional regression, and the corresponding ratings are generated from multilevel regression to highlight the effect of shrinkage to the mean.\relax }}{54}{figure.caption.27}
\contentsline {figure}{\numberline {A.2}{\ignorespaces Offensive and Defensive team ratings for the 2020 NHL season. The points with the team labels next to them are ratings generated by traditional regression, and the corresponding ratings are generated from multilevel regression to highlight the effect of shrinkage to the mean.\relax }}{55}{figure.caption.28}
\contentsline {figure}{\numberline {A.3}{\ignorespaces Offensive and Defensive team ratings for the 2020 MLB season. The points with the team labels next to them are ratings generated by traditional regression, and the corresponding ratings are generated from multilevel regression to highlight the effect of shrinkage to the mean.\relax }}{56}{figure.caption.29}
\contentsline {figure}{\numberline {A.4}{\ignorespaces Offensive and Defensive team ratings for the 2020 NFL season. The points with the team labels next to them are ratings generated by traditional regression, and the corresponding ratings are generated from multilevel regression to highlight the effect of shrinkage to the mean.\relax }}{57}{figure.caption.30}
