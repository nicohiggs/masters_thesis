- intro and abstract similar to paper

- from paper plus links to earlier discussed motivations for multi-level modeling

\section{Multilevel Model}

- similar to paper
- may need to build this up so as to include all data experiments or a more general version to refer back to

We infer home advantage by fitting a regression model to predict the points scored in each game while adjusting for relative team strengths and home advantage. We adjust for relative team strengths by modelling both an offensive rating and a defensive rating for each team. We argue this better represents real differences between teams and allows the model to better infer if a team performs better or worse when playing at home by measuring its performance relative to its average offensive performance versus its opponents average defensive performance. This section describes in detail the parameters of the model, their interpretation, and how we fit the model.

We aimed to build a parsimonious model to infer home advantage for each league while adjusting for relative team strengths and accounting for uncertainty in the data and parameter estimates. We needed a method that was robust to smaller sample sizes because we only had one COVID-19 adjusted season for each league to compare to and because this sample becomes smaller as you include more parameters which splits the data into smaller groups. We also wanted to be able to quantify the uncertainty in our parameter estimates. To address these concerns we adopt a Bayesian multi-level regression model framework building upon previous work \mbox{\cite{Baio2010} \cite{Glickman1998} \cite{Lopez2018} \cite{Benz2020}} that allows for pooling results across all teams to infer home advantage. The partial-pooling of multi-level regression modelling allows us to separate the effects of individual teams offensive and defensive strengths from their group level means and helps prevent overfitting by adjusting parameter estimates through a process commonly referred to as "shrinkage to the mean" \mbox{\cite{Gelman2014} \cite{Gelman2006} \cite{McElreath2020}}. We argue the pooling of data across each teams results to better handle smaller sample sizes while preventing overfitting, and the ability to quantify the uncertainty in parameter estimates makes Bayesian multi-level regression an ideal choice for this task.

We model the response variable of the number of points scored by each team in each game as Negative Binomial:

where \(y_{ij} = [y_{i1}, y_{i0}]\) is the vector of observed points scored in game \(i\) by the home (\(j=1\)) and away (\(j=0\)) teams and \(\mu_{ij} = [\mu_{i1}, \mu_{i0}]\) are the goal expectations of the home and away teams in game \(i\). The \(\alpha\) parameter allows for the flexibility of fitting to overdispersed data where the variance is much greater than the mean. In our experiments we have found that defining \(\alpha\) as a fraction of \(\mu_{ij}\) led to better sampling and model fit. Thus, we define \(\alpha_{ij} = \mu_{ij} * \lambda\) and then sample \(\lambda\) when fitting the model. We model the logarithm of goal expectation as a linear combination of explanatory variables:

\begin{equation} \label{eq:expected points}
\begin{split}
\text{log}(\mu_{i1}) &= \gamma_{sp} + \beta_{sp} + \omega_{sh[i]} + \delta_{sa[i]} \\
\text{log}(\mu_{i0}) &= \gamma_{sp} + \omega_{sa[i]} + \delta_{sh[i]}
\end{split}
\end{equation}

where \(\gamma_{sp}\) is the intercept term for expected log points in season, with \(s = [0, 1, 2, 3, 4]\) corresponding to the 2016, 2017, 2018, 2019, and 2020 seasons respectively. The subscript \(p\) indicates regular season (\(p=0\)) or playoffs (\(p=1\)). For the results in Figure \ref{fig:ha_pooled}, all previous seasons are combined (\(s=0\)) and compared to the COVID-19 adjusted season (\(s=1\)). Home advantage is represented by \(\beta_{sp}\) with \(s\) and \(p\) the same as the intercept. The offensive and defensive strength of the two teams are represented by \(\omega\) and \(\delta\). The nested indexes \(h[i]\) and \(a[i]\) identify the teams playing at home and away respectively and we use this nested notation to emphasize the multi-level nature of these parameters as they are modelled as exchangeable from a common distribution \cite{McElreath2020} \cite{Gelman2014} \cite{Gelman2006}. This enables pooling of information across games played by all teams in a league and results in mixing of the observable variables \((y_{ij})\) at this higher level which accounts for correlation in home and away points scored in each game \cite{Baio2010}.

In this model formulation we are estimating different home advantage parameters for the regular season and playoffs as well as for each individual season. The primary motivation for this is because the NHL and NBA COVID-19 bubbles essentially only occurred during their playoffs and we therefore want to separate home advantage during the playoffs for a more direct comparison. Modelling in this way also addresses potential questions of whether home advantage changes each year or remains constant. Our results in Figure \mbox{\ref{fig:ha_pooled}} are from estimating one home advantage parameter prior to COVID-19 and one afterwards. We then show the results of modelling home advantage separately for each season and show the results in Figure \mbox{\ref{fig:ha_main}} which reveal some interesting differences as discussed in the Results section.

In (\ref{eq:expected points}) we see that the home team's goal expectation is a linear combination of the home team's offensive strength and the away team's defensive strength as well as a constant home advantage. Conversely, the away team's goal expectation is a linear combination of the away team's offensive strength and the home team's defensive strength with the home advantage parameter noticeably missing. There is no index for league because, although we use the same model consistently across each league, we fit a separate version for each league.

This model formulation results in the intercept representing the logarithm of the overall average of points scored with \(exp(\beta_{sp}), exp(\omega_{sh[i]}),\) and \(exp(\delta_{sa[i]})\) representing multiplicative increases or decreases to the average points scored to determine the expected points scored for an individual game. This can be seen by considering:

\begin{equation}
\begin{split}
\text{log}(\mu_{i1}) &= \gamma_{sp} + \beta_{sp} + \omega_{sh[i]} + \delta_{sa[i]} \\
\mu_{i1} &= \text{exp}(\gamma_{sp} + \beta_{sp} + \omega_{sh[i]} + \delta_{sa[i]}) \\
\mu_{i1} &= \text{exp}(\gamma_{sp})*\text{exp}(\beta_{sp})*\text{exp}(\omega_{sh[i]})*\text{exp}(\delta_{sa[i]})
\end{split}
\end{equation}

For example, a home advantage parameter of \(\beta = 0.25\) would result in multiplying the average points scored by \(\text{exp}(0.25) \approx 1.28,\) which can be interpreted as an increase of about 28\% in expected points scored by the home team in a game between teams with relative offensive and defensive strengths \(\omega_{sh[i]}\) and \(\delta_{sa[i]}\) respectively.

\subsection*{Model Fit in PyMC3}

The models are fit using PyMC3, an open source probabilistic programming language that allows us to fit Bayesian models with their implementation of a gradient based Hamiltonian Monte Carlo (HMC) No U-Turn Sampler (NUTS) \cite{pymc3}. As in other previous work \cite{Baio2010} \cite{Benz2020}, we use Bayesian modelling and fitting approaches to allow us to incorporate some prior baseline knowledge of parameters as well as better quantifying uncertainty in the interpretation of parameter estimates.

The Bayesian approach means we need to specify suitable prior distributions for all random parameters in the model. The prior distributions for parameters in our model are:

\begin{equation}
\begin{split}
\gamma_{sp} &\sim \mathcal{N}(\theta^*, \sigma^{2*}) \\
\beta_{sp} &\sim \mathcal{N}(0, 1) \\
\lambda &\sim \text{Uniform}(0, 1000) \\
\omega_s &\sim \mathcal{N}(0, \sigma_{s\omega}) \\
\delta_s &\sim \mathcal{N}(0, \sigma_{s\delta}) \\
\sigma_{s\omega} &\sim \text{HalfNormal}(1) \\
\sigma_{s\delta} &\sim \text{HalfNormal}(1)
\end{split}
\end{equation}

where \(\theta^*\) is the logarithm of the average points scored, and \(\sigma^{2*}\) is the logarithm of the variance of points scored, over the regular seasons and playoffs of the league being modelled. We note that we found \(\gamma_{sp}\) fits close to \(\theta^*\) even when using a weakly informative prior, but we keep this formulation as it maintains the spirit of using prior information in Bayesian analysis. We allow \(\lambda\) to potentially be large for instances where there is no overdispersion in the outcome variable because a large \(\lambda\) results in a large \(\alpha_{ij}\) which makes the Negative Binomial distribution become similar to a Poisson distribution.

The model is fit using PyMC3's NUTS sampler using 4 chains of 2000 iterations with 1000 tune steps for a result of 8,000 samples from 12,000 total draws. It is standard practice to check convergence with the \(\hat{R}\) statistic from \cite{Gelman1992} \cite{Brooks1997}.  Each model fit produced \(\hat{R}\) statistics of 1.00 with no divergences \cite{Betancourt2017}.

\section{Negative Binomial}

- tables and figures like paper for justification

\section{Experiments}

- synthetic data and then real data showing overfitting and the value of shrinkage
We create and compare three models in order to show the benefits of how multilevel modelling partially pools information across groups to improve model fit while preventing overfitting. The first model is referred to as a \textit{completely pooled} model where the data from all groups is completely pooled into one overall average to be used. The second model is referred to as a \textit{no pooling} model where essentially a separate regression fit is made for each group ignoring the information from other groups. Finally a multilevel model which is also referred to as a \textit{partially pooled} model where essentially separate regressions are fit for each group but with group parameter estimates shrunk to the overall mean relative to sample size and group variances so as to help prevent overfitting. In the completely pooled model there is essentially one global average used for all groups and in this way the model has high bias and ignores the differences amongst groups. In the no pooling model each group has its own parameter fit, but completely ignores the data from other groups and how they are fit resulting in lower bias and a better fit to the data but at the risk of overfitting. The partially pooled model is a balance between these two extremes allowing for a better model fit than the completely pooled model while better protecting against overfitting than the no pooled model.

To compare the models we created a synthetic dataset for which we know precisely how the data was generated. Half of the data will be used to train each model and evaluate training fit, and then the other half will be a hold out set that will be used to evaluate testing fit on unseen data. We will then repeat the same fitting and evaluating procedures for each model on the actual sports datasets used for home advantage inference in this thesis.

To generate the synthetic dataset we use a similar data generating process to \cite{Benz2020}. We generate team attacking and defensive strengths (\(\alpha_t \sim N(0, 0.35)\) and \(\delta_t \sim N(0, 0.35)\)) for \(t=1,...,20\) teams. A game is then simulated by using model (cite model number) to randomly generate the number of points scored by each team with the home advantage parameter set to \(\beta = 0.25\) and intercept \(\mu = 0\). We simulate a regular season by simulating 2 games between each pair of teams, one home game for each team, resulting in \(38\) game appearances for each team and a total of \(380\) games. The sports datasets are the same used in this thesis and are described in (data section??)

While the primary use of Bayesian models is to have a distribution for not only parameter estimates but predictions as well, we take the mean of the sample distribution of predictions for each model as their respective predictions to be compared to the actual data. The models are then compared by their mean squared error (MSE), with the results shown in Table ??. There are two key features that stand out in the results in Table ??. First, both the no pooling model and the partial pooling model have lower MSEs in both training and testing. This indicates that the completely pooled model is underfit and that the no pooling and partial pooling model do improve model fit by including group parameters (team ratings in this specific case). Second, the no pooling model provides the best model fit on the training data, but provides worse model fit than the partial pooling model on the test data. This shows the effect of regularization via shrinkage to the mean that was theoretical discussed earlier in section ??. These results give empirical evidence that multilevel models and their partial pooling provide improved model fit while protecting against overfitting, and is the reason we opted to use a multilevel model as the model of choice for inferring home advantage in this thesis.

- the experiments from the paper
- negative binomial
Since point totals in sports are positive integers, the Poisson distribution is a natural choice for modelling their outcomes. The effectiveness of the Poisson distribution for modelling point totals has been shown in several works analyzing European football data \cite{Karlis2003} \cite{Baio2010} \cite{Benz2020}. One shortcoming of the Poisson distribution is that it only has one parameter and this leads to the strong assumption that the mean is equal to the variance. For low scoring sports like European football and hockey, this is usually a fine assumption. However, this is an invalid assumption for several of the sports we analyze in this paper. Table \ref{tab:loo} reports the dispersion statistic \(\sigma_p\). The dispersion statistic represents how much greater the variance is than the mean while adjusting for sample size and model complexity and is computed as  \(\chi^2/(n-p)\) for each league, where \(\chi^2\) is the Pearson chi-squared statistic of the point totals data, and \(n-p\) are the degrees of freedom with \(n\) representing the sample size of the point totals data and \(p\) representing the number of predictors in our model. The commonly suggested threshold, \(\sigma_p > T\), for determining when a Poisson model is no longer appropriate is around \(1.2 < T < 2\) \cite{Payne2018} \cite{Cameron1990}. Table \ref{tab:loo} shows the NBA, MLB, and NFL having potential overdispersion in their point totals and thus, the Poisson distribution is likely inappropriate and less effective. We instead opt for using the Negative Binomial distribution because it has an extra parameter \(\alpha\) that gives greater flexibility and better model fit to data that is overdispersed while still adequately fitting models without overdispersion.

To establish the efficacy of the Negative Binomial distribution in our model, we fit and compare models using the Poisson and Normal distributions across each league. We fit Poisson and Normal regression models by changing the likelihood of the model in (\ref{eq:likelihood}) to \(y_{ij} | \mu_{ij} \sim \text{Pois}(\mu_{ij})\) for the Poisson regression (and subsequently drop \(\alpha\) from the rest of the model as it is not needed), and \(y_{ij} | \mu_{ij}, \sigma^2 \sim \mathcal{N}(\mu_{ij}, \sigma^2)\) for the Normal regression (and use a weakly informative prior \(\sigma^2 \sim \text{HalfNormal}(50)\)). Otherwise the models are identical and their interpretation remains the same as is discussed in the Methods section.

We evaluate the models across each league by estimating the out-of-sample predictive fit via leave-one-out cross-validation (LOO). Following the work of Vehtari \cite{Vehtari2016} we approximate LOO using Pareto-smoothed importance sampling (PSIS) and report the results in Table \ref{tab:loo}. We note here that we also used the widely-applicable information criterion (WAIC) \cite{Watanabe2010} but found the results to be nearly identical and the conclusions the same.

The results can be seen in figure ?? and table ?? and are discussed in section ??.

-home advantage inference
To make inferences about home advantage prior to and during the COVID-19 pandemic we fit model ?? described in section ??. The fitting of this model results in parameter estimated for home advantage across each of the four leagues analyzed for four seasons prior to and one season during COVID-19. We then analyze the trends and differences across seasons as well as leagues in order to make inferences and draw conclusions about the impact of home advantage in these sports. The results can be seen in figure ?? and are discussed in section ??.

- maybe the synthetic ha simulations

\section{Model Evaluation}

It is not enough to simply create a model and fit it to a dataset. There are infinitely many models that could be created, and some models will be better than others depending on the context or objective of the model. Thus, it is important we evaluate how well a given model fits a dataset and gauge its predictive performance. This allows us to understand how effectively a given model fits our dataset and it gives us a way to select the "best" model from among several models. This section describes how we evaluate the efficacy of Bayesian models via posterior predictive checks and cross validation.

-PPC

Posterior predictive checks (PPCs) give us a way to visually check how well a model fits the dataset. Since Bayesian models are generative, we can use the fitted model to simulate values and then observe how closely these generated values resemble the observed dataset. If the distribution of simulated values closely resembles the observed dataset then we say that the model is "well-specified" or is a good fit to the dataset. If the distribution of simulated values does not closely resemble the observed dataset then we say that the model is "misspecified" or is a bad fit to the dataset. It is noteable that PPCs do not only reveal that a model is potentially misspecified but often also give insight into how to potentially improve the model by visually seeing for which data points the model is struggling to fit well. In this way PPCs are not only useful for model evaluation but for model building as well. It was PPCs that lead to the insight of using the Negative Binomial distribution over the Normal and Poisson distributions to fit the sports datasets.

- cross validation
It is natural to desire a model that fits the dataset as well as possible. However, it is possible to create models that fit a specific dataset so well that they fail to generalize to the larger population of which the dataset is only a sample. When this happens we say that a model is "overfit". An overfit model is one that provides a good fit to the dataset and is effective at retrodicting the dataset (predicting the dataset it was trained on). While these properties are generally desirable, they can come at the cost of the model only fitting and retrodicting the dataset it was trained on and then performing much worse on unseen data or future scenarios for which the model was ideally created for. Thus it is important that we not only evaluate a model on the basis of how well it fits and retrodicts the dataset it was trained on, but that we also try to estimate how well the model fits the larger population and its ability to predict data that it was not trained on. 

The simplest way to approximate how well a model will perform on unseen data is to "hold-out" a portion of your dataset referred to as a test-set, fit your model to the rest of the dataset referred to as the train-set, and then check the fit and predictive performance of the model on the test-set. If a model performs noteably worse on the test-set, then you conclude the model is overfit and you reject it even though it may be the highest performing model on the train-set. The objective is to create a model that has the best fit and predictive performance on the test-set and does not have a noticeable drop off in performance from the train-set to the test-set. While this method is straightforward and generally effective it does have some drawbacks. The size of the train-set used to train the model is now smaller and may be too small to accurately reflect the model if it were trained on a larger dataset. The selection of which data is divided into the train and test sets may also bias the model. For example, if an essential group or cluster of similar data points are all put into the test set then the models poor performance on the test set could be misleading. Cross-validation is an attempt to alleviate these concerns and improve upon this method.

Cross-validation partitions the dataset randomly into K-many sets (with $K \geq 2$). It then uses one of the sets as the test set, and combines the rest into the train set. The model is fit and then evaluated on the test set. This process is then repeated using each of the K sets as the test set and then using the average performance across all test sets as the estimate for out of sample performance. In this way the entire dataset is used in both training and testing helping to alleviate small data concerns. As you increase the value of K you also increase the size of the train set which gives not only a better model fit but is more likely to overfit if the model itself is prone to overfitting, something you would desire to find out. However, increasing the value of K also means you need to train and test the model more times. This is most noticeable when considering leave-one-out cross-validation (LOO-CV), where K is the size of the dataset. In this case you train the model on all but one data point and then test on the one data point that was left out, and repeat for each data point. While this gives the best estimate of out of sample performance, it requires you to train the model a large number of times. For datasets that number in the thousands or more in the current big data era this can become computationally infeasible. The usual way to deal with this is to reach a compromise by using a smaller value of K, usually 5 or 10, but in Bayesian inference there have been some advancements in the approximation of estimating LOO-CV performance that are related to information theory.

- information theory, WAIC and PSIS-LOO
(introduce information theory by briefly mentioning claude shannon?)
The goal of model fitting and model evaluation is to fit a model that maximizes performance relative to some objective or criterion. While we have described the perils of overfitting and how cross-validation arose as a method to prevent it, this is like defining our objective or criterion which we want to maximize to include regularization. It turns out that there is a branch of mathematics known as Information theory that defines a criterion and a way to maximize it that points to similar goal known as the out-of-sample \textit{deviance}. In information theory, information is formalized into a mathematical formula for information entropy as follows:

\begin{equation}
H(p) = -Elog(p_i) = - \sum_{i=1}^{n} p_i log(p_i)
\end{equation}

In this way, information entropy can be thought of as measuring the uncertainty contained in a probability distribution as the average log-probability of the event (cite Mclreath stat rethink).

- is all this discussion of information theory needed? From here we still need to introduce KL divergence, then deviance, then AIC -> WAIC and then we get to PSIS-LOO

The work of (cite Vehtari) introduced an efficient computation of LOO-CV from MCMC samples without requiring the repeated re-fitting of the model to data. The approximation is based on importance sampling where the importance weights are stabilized using a method known as Pareto-smoothed importance sampling (PSIS). The PSIS-LOO estimate is also an estimate of the out-of-sample relative K-L divergence (i think i may need to define kl divergence here). In practice is it extremely similar to the Widely Applicable Information Criterion (WAIC) that also estimates the out-of-sample relative K-L divergence. The models that are fit and compared in this thesis all gave near identical results for both PSIS-LOO and WAIC and we have chosen to report just PSIS-LOO in the results section.

- finish by explicitly stating how each data experiment is evaluated