\section{Home Advantage}
\subsection{Related Work}

The initially most well known and cited work on home advantage in sports was done in 1977 by Schwartz and Barsky \cite{Schwartz1977} who analyzed and found home advantage to exist in professional hockey, basketball, baseball and football. In \cite{Courneya1992} the authors accept home advantage as a real phenomena after reviewing the relevant literature and argue for a framework that focuses on game location, psychological states, behavioral states, and performance outcomes to try to understand the underlying causes of home advantage. Follow up work a decade later by Carron et al. \cite{Carron2005} reviewed the literature and concluded that home advantage was still present in both amateur and professional sports, in both individual and team sports, across genders, and across time. More recent works \cite{Pollard2005a} \cite{Gomez2011} confirm the continued existence of home advantage in the North American professional leagues we are considering in this study: the NHL, NBA, NFL, and MLB. In general, older studies on home advantage tend to use correlation methods of aggregated full season statistics (e.g. combining all teams home wins into one home win percentage to see if it is above 50\%), whereas more recent studies more often build statistical regression models from game level data that adjust for additional factors, such as relative team strengths, and try to infer the effect of the home advantage parameter on the regression model.

There have been several studies analyzing home advantage in the context of COVID-19 adjusted seasons; however, nearly all of them have focused exclusively on European Soccer leagues. In \cite{Benz2020} thirteen such works are summarized, of which only two used correlation methods and the other eleven made use of regression analysis to infer the change in home advantage. Benz and Lopez themselves use a bivariate Poisson regression model to infer home advantage, thus making for twelve of the fourteen studies making use of regression analysis. Ten of these studies found a drop in HA during the COVID-19 adjusted seasons, with the other four reporting mixed results where HA dropped in some leagues but not in others. We are only aware of one academic article looking at home advantage in the COVID-19 adjusted seasons for the NBA \cite{McHill2020} where the authors found presence of home advantage prior to the NBA's bubble and argue for teams travel schedules having the most notable impact. As of this writing there are no academic papers examining home advantage during the COVID-19 adjusted seasons for the NHL, NFL, or MLB, although several online blog articles exist (cite?? or leave out?) most of which take a quick cursory glance at raw home win percentages and do not account for team performance relative to strength of opponents as is done in the work summarized in \cite{Benz2020}. This paper is a first look at using regression to infer home advantage through team performance while adjusting for quality of opponents instead of only looking at aggregated statistics such as win percentage.

There is a growing body of work in sports analytics that turns to building statistical models to measure relative team strengths while accurately predicting game outcomes. These works have their roots found in Bradley-Terry models \cite{Bradley1952} and Bayesian state-space models \cite{Glickman1998}. Further advancements and examples from the NHL, NBA, NFL, and MLB are comprehensively summarized in \cite{Lopez2018} and follow a form similar to the model in \cite{Baio2010} as Bayesian methods generally offer more flexibility to be able to extend and customize these models and are generally more stable when fitting the models to data \cite{GlickmanText2017} while better capturing the uncertainty in estimating parameters opposed to classical point estimates and p-values which are increasingly under criticism in modern science. While most of this work was developed with a focus on predicting game outcomes and measuring team strengths, they often include a term to adjust for home advantage and as such can be re-purposed to be used to infer home advantage as is done in the majority of works summarized by \cite{Benz2020}. In this paper we aim to take the first attempt to use these methods to infer home advantage during the COVID-19 adjsuted seasons of the NHL, NBA, NFL, and MLB.

In \cite{Lopez2018} the authors show the improved efficacy of the Poisson distribution instead of the more common Normal distribution \cite{GlickmanText2017} for modelling points scored by each team in each game. In \cite{Benz2020} the authors follow the work in ntzoufras arguing for the use of a bivariate Poisson distribution that accounts for small correlation between two teams scoring and show its efficacy over ordinary least squares regression in inferring home advantage via simulations. However, as is shown in \cite{Baio2010} there is no need of the bivariate Poisson when working within the Bayesian framework because hierarchical models of two conditionally independent Poisson variables mix the observable variables at the upper level which results in correlations already being taken into account. In \cite{Baio2010} the authors argue for more complex methods to limit the shrinkage of their hierarchical model as their data was from leagues with a large range of team strengths. We follow \cite{Lopez2018} who showed that the "big four" North American Professional leagues are very close in team strength and thus do not reduce the shrinkage from our hierarchical model.

This next part may need to be taken out...
The challenge with methods that look at correlations among raw statistics such as home win percentage is that they fail to account for other factors such as relative team strengths. For example, a weaker team may have poor home win percentage because they have a poor overall win percentage. That same team; however, may perform better at home than they do at other stadiums whilst still losing to stronger opponents and vice versa. This discrepancy can be further impacted by imbalanced schedules. In the professional leagues we consider, teams generally do not face each each opponent the same number of times and do not face the same strength of opponents at home and away in a perfectly balanced manner. While studies often recognize this discrepancy, they often claim that it is a small effect that can be ignored \cite{Pollard2005a} without showing evidence. We argue that these issues and any debate over how much of an effect they have is most reliably mitigated by accounting for other factors, most notably team strengths, when trying to infer home advantage. Regression analysis methods are most often used for precisely their ability to account for multiple factors when performing inference, and as such they are most appropriate for our focus of analyzing home advantage.

To aid in showing the benefit of using regression to adjust for relative team strengths when trying to infer home advantage, we present a simulation of game outcomes and the difference between raw home win percentage and our models inferred results. We use a similar data generating process to \cite{Benz2020}. We generate team attacking and defensive strengths (\(\alpha_t \sim N(0, 0.5)\) and \(\delta_t \sim N(0, 0.5)\)) for \(t=1,...,20\) teams. A game is then simulated by using model (cite model number) to randomly generate the number of points scored by each team with the home advantage parameter set to \(\beta = 0.25\) and intercept \(\mu = 0\). We simulate a regular season by simulating 4 games between each pair of teams, two home games each, resulting in \(76\) game appearances for each team for a total of \(760\) games, which most closely resembles the NHL and NBA  regular season schedules. Then we simulate the top 16 teams playing a knock-out tournament bracket consisting of best 4 out of 7 matches, again closely following the NHL and NBA formats. We then compare using raw home win percentage to predict existence of home advantage compared to model (cite model). The results can be seen in figure (??).

There does exist work in baseball analytics analyzing the stability of home win percentages (cite, those articles found on your phone); however, they look at the stability over decades at a time. In our context we simply do not have decades worth of covid restricted professional games. Given the "small data" of our problem, we maintain that Bayesian inference is best suited for this task.


- high level explanation/transition to why bayesian methods are what I used

\subsection{Contribution}

We adopt a Bayesian framework to develop a Negative Binomial regression model that adjusts for relative team strengths while inferring home advantage. We choose this approach for two main reasons. First, alternative methods that rely on correlations among raw statistics, such as home win percentage, fail to account for other factors such as relative team strengths. Our regression approach can infer changes in team performance while adjusting for quality of opponents. Second, the Bayesian framework gives more interpretable results and more flexibility in model building than classical regression methods. The Bayesian framework results in distributions for the estimates of each parameter in our model. This allows us to analyze these distributions directly to determine the probability a parameter is greater (less) than a certain value or that it exists in a specific interval, avoiding the confusion that often arises interpreting p-values and confidence intervals.

By examining the resulting home advantage parameter estimates of our model from before and during the COVID-19 pandemic, we can draw conclusions about the existence of the home advantage phenomenon and provide new evidence for its potential causes. We hypothesize that home advantage is a real phenomenon, thus we expect its parameter estimate to drop during the COVID-19 seasons relative to before the COVID-19 seasons. We are also interested in examining if any differences in relative changes in home advantage exist across the leagues as some leagues had different COVID-19 restrictions which could affect home advantage differently. We also show that point totals in North American professional sports are prone to overdispersion, thus, the Negative Binomial distribution allows for better model fit than the more common Poisson and Normal distributions used in regression analyses.

\section{Bayesian Inference}

\subsection{Introduction}

- basic intro and explanation of bayes theorem
	- p(a|b) = p(b|a)p(a)/p(b) from p(a and b) <- probably should switch to theta's and x's
	- perhaps the breast cancer example
	
Bayes theorem is named such after revered thomas bayes. It was actually named posthumously after his friend found some of his work and had it published after his death. Thomas Bayes was trying to solve an inverse probability problem...

Bayes theorem can be stated simply as:
\begin{equation} \label{eq:bayes_theorem}
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
\end{equation}
This equation can be derived simply from basic rules of probability and conditional probability, namely that $P(A\cap B) = P(A|B)P(B)$ and equivalently $P(A \cap B) = P(B|A)P(A)$. One can then simply substitue and isolate $P(A|B)$ to arrive at equation \ref{eq:bayes_theorem}.

In equation \ref{eq:bayes_theorem}, each part of the equation is often referred to and interpreted differently. $P(B|A)$ is referred to as the \textit{likelihood}, and is understood in the same way likelihood is understood in traditional frequentist statistics. $P(A)$ is referred to as the \textit{prior}, and can be understood as the prior belief we have for the value of $A$. $P(B)$ is referred to as the \textit{data}...

As is often the case in science, the name of a discovery is usually not the actual discoverer or first user. In the case of Bayes theorem, it is known that (a century earlier?) Piere Simone Laplace was using what we now refer to as Bayes theorem to solve an inverse probability problem of his own. In his work...


- bayes vs frequentist
	- mention thomas bayes and laplace
	- ra fischer and the 'smear' campaign
	
Despite its early roots, bayesian statistics took a back seat during the 20th century. Most prominent statisticians of the time did not like the "subjectivity" of specifying a prior that could potentially influence the results of inference. In particular, the most prominent statistician of the 20th centry, RA Fischer, was a vocal opponent of "subjectivity". Many other prominent statisticians such as ... also comdemned the use of bayesian statistics.

The other issue with bayesian statistics that prevented it from becoming a more mainstream technique for inference is its computational difficulties. Although Bayes theorem is rather simple to state, it often leads to requiring the computation of integrals that is either exceedingly difficult or outright impossible analytically. In order to overcome this, a user is left with essentially two options to approximate the posterior distribution. The first is to change the prior and likelihood distributions into ones that are solvable analytically; this is known as choosing a conjugate prior. The issue with this method is that you are forced to use different prior and likelihood distributions than you original wanted and thus are no longer accurately representing the problem. The other method for approximation is to use a sampling procedure, such as a markov chain monte carlo (MCMC) procedure, to generate enough samples of the target posterior distribution to reasonably approximate it. The issue with this method is that MCMC sampling can be very slow taking a long time to converge and the results can be noisy and do not have any guarantees for the accuracy of the approximation.
	
- brief mention of why bayes is making a combeback now
	- real world success (mostly military)
	- advances in computing power
	- advances in mcmc theory (hmc)

Despite bayesian statistics being pushed to obscurity in the early-mid 20th century via a smear campaign and the difficulties in its computations, that latter part of the 20th century in to the 21st centruy is seeing a rise in the popularity and use of bayesian statistics through its real world successes, advances in computing power, and advances in MCMC theory.

- need real world examples from 'the theory that wouldn't die' book
- need some hard stats on advances in computing power and sampling
- need to cite beatancourt's work on HMC and future developments

\subsection{Multilevel Modeling}

MOTIVATION
- a generalization of regression methods with many use cases such as prediction, data reduction, and causal inference from experiments and observational studies.
- called multilevel or hierarchical for two reasons. 1) the structure of the data (e.g. students clustered within schools within districts within states within countries...). 2) the model itself has its own hierarchy with parameters of the within-groups regression at the bottom, controlled by the hyperparameters of the upper-level model.
- multilevel modeling can be viewed as a trade-off between two extremes: complete-pooling and no-pooling. Complete-pooling is when an overall average is used and variations among groups/categories within the overall data are ignored, thus the data are “completely pooled”. No-pooling is when separate models/averages for each individual group/category are used and any correlations or dependencies among the groups are ignored, thus the data is not “pooled” at all. In this view, multilevel models are seen as “partially-pooled” where their estimates can be thought of in a simplified way as a weighted average of the complete-pooling and no-pooling extremes. For groups/categories with fewer data points the multilevel model weighs the complete-pooling more, and for groups/categories with more data points the model weighs the no-pooling estimates more. This results in what is commonly referred to as “shrinkage” whereby partially pooled estimates are essentially the no-pooling estimates that have been “shrunk” toward to complete-pooling estimate, or “shrunk toward to the mean”. The amount of shrinkage depends on the samples-sizes, the variation within groups, and the varition between groups.
- outperforms classical regression in predictive accuracy since multilevel modeling includes least squares regression as a simple case. Generally considered essential for prediction, useful for data reduction, and helpful for causal inference.
- multilevel modeling can be viewed as a “white-box” method whereby each part of the model can be fully interpreted, understood, and customized. This makes it ideal for inference. Furthermore, multilevel models are Bayesian graphs which means that Judea Pearl’s causal calculus (or do-calculus) can be used to infer causality. This makes multilevel models useful beyond predictions alone. In contrast, this is something that many machine learning methods such as neural networks and decision trees, can not do.
- multilevel modeling allows separating estimates of predictive effects of an individual predictor and its group-level mean (usually referred to as “direct” and “contextual” effects of the predictor).
- several motivations for their use:
- learning about treatment effects that vary: how does y change when some x is varied (with all other inputs held constant)? Often its not the overall effect of x but rather how this effect varies in the population or among groups/categories of interest.
- using all the data to perform inferences for groups with small sample size: at one extreme classical estimation can be useless if the sample size is small in a group/category, and at the other a classical regression ignoring group-level variation can be misleading as well. Multilevel modeling (partial pooling) compromises between the overall noisy within-group estimates (no-pooling) and the oversimplified regression estimate that ignores group indicators (complete-pooling).
- predictions: as discussed above, the partial-pooling or “shrinkage” effect of multilevel modeling is a form of regularization that protects from underfitting (complete-pooling) and overfitting (no-pooling) to produce more accurate predictions on unseen data (the holy grail of machine learning).
- analysis of structured data and more efficient inference for regression parameters: many datasets have an inherent multilevel/hierarchical structure (e.g. students within schools, patients within hospitals, laboratory assays on plates, elections in districts within states, or data from cluster sampling etc.). Even “simple” cross-sectional data can be placed in a larger multilevel context. For example, many datasets initially thought to be “big data” often become “small data” once you being sub-dividing them into more and more groups/categories. For example, opinion polls trying to predict who you vote for based on age, race, income, location, interests etc. Each split leaves you with smaller and smaller groupings that have the potential for better model fit(more predictors) at the risk of overfitting (small samples within groups/categories).
- including predictors at two different levels: You can specify models that have individual level predictors and group level predictors. For example, in estimating radon levels in houses you could have measurements at the individual level (individual houses, indicator if the sensor is in the basement, etc.) and then predictors at the group level (county-level uranium readings) and using both together provides better model fit than separating them. Multilevel modeling avoids problems in classical regression such as colinearity when trying to include group-level indicators as well as group-level predictors in the same model.
- getting the right standard error (accurately accounting for uncertainty in prediction and estimation): To get an accurate measure of predictive uncertainty, one must account for correlation of the outcome between groups/categories/predictors (e.g. forecasting state-by-state outcomes in US election, one must account for correlation of outcome between states in a given year). Sometimes the uncertainty in estimation is of interest rather than the estimate itself. Sometimes predictions require multilevel modeling, such as when making predictions for a new group. For example, consider a model of test scores for students within schools. You could model school-level variability in classical regression (or another machine learning model such as decision trees or neural nets) with an indicator for each school. But it is impossible in this framework to make a prediction for a new student in a new school, because there is no indicator in the model for this new school. This type of problem is handled seamlessly when using the multilevel framework.

- partial pooling estimates
It is potentially helpful to consider a simplified estimate from a partially pooled model in order to understand how partial pooling works. Consider a model that has group indicators but no other predictors. In this case the partially pooled model will generate predictions for each group by constructing a weighted average of the group level mean and the overall mean. Mathematically it would be constructed like this:

\begin{equation}
\hat{\alpha}_j^{multilevel} \approx \frac{ \frac{n_j}{\sigma_y^2} \bar{y}_j + \frac{1}{\sigma_{\alpha}^2} \bar{y}_{all} }{ \frac{n_j}{\sigma_y^2} + \frac{1}{\sigma_{\alpha}^2} }
\end{equation}

where $\hat{\alpha}_j^{multilevel}$ is the estimate from the multilevel model for the jth group. It is the weighted average of the jth groups average ($\hat{y}_j$) and the average of all groups combined ($\hat{y}_{all}$). The weights are determined by the within-group variance ($\sigma_y^2$), the sample size of the jth group ($n_j$), and the variance among the groups ($\sigma_{\alpha}^2$). In this way, the larger (smaller) the sample size of the jth group and the lower (greater) the within-group variance leads to a larger (smaller) weight placed on the jth group average for the final estimate. The smaller (larger) the variance among the groups leads to a larger (smaller) weight placed on the overall average for the estimate of the jth group. This view makes it clear that the estimates from a multilevel model will compute a group estimate in a similar way to a more traditional regression model but will then shrink that estimate toward the overall mean weighted by the groups sample size, the within group variance, and the among group variances.

Equation ?? has a $\approx$ symbol rather than $=$ symbol because it is only in a few mathematically convenient cases, such as conjugate priors, that the group level estimate would precisely reduce to the formula in equation ??. When you include more predictors, more mathematically complex transformations and other engineered features, and you use more varied probability distributions that do not result in conjugate priors, then the estimates are no longer mathematically tractable and instead the user must turn to approximation methods such as MCMC to generate the estimates. However, even in such complex cases where we can not work out the estimates analytically they still in practice function in the same way as outlined by equation ??.

- more transition to mcmc?

\subsection{Markov Chain Monte Carlo}

For most models of practical interest, exact inference is intractable, and so we have to resort to some form of approximation (pattern recognition and machine learning 2006).

The primary end goal of Bayesian inference is computing the posterior distribution. It is with the posterior distribution that we can perform inference and answer questions about quantities of interest. The issue is that computing the posterior distribution for nearly all but the simplest of models is not only difficult but often impossible. That is to say that we can not derive a closed form mathematical expression that represents the posterior distribution. We can, however, approximate the posterior. Researchers have developed many different methods of numerical approximation which can be employed to approximate the posterior distribution in Bayesian inference. For the models considered in this thesis we employ the most popular method, Markov Chain Monte Carlo (MCMC), to approximate the posterior to make our inferences from.

- metnion of concentration of measure and how most optimization algorithms struggle with that here and hence mcmc sampling is the best we currently have

The most common method to approximate computing a desired probabilistic quantity is to repeatedly draw independent samples from the probability distribution and to then average over those samples to approximate the quantity of interest. This is known as Monte Carlo sampling.  Just as statisticians traditionally aim to draw independent samples in order to estimate desired quantities, such as the mean, variance, or specific quantiles, about a target population, Monte Carlo sampling aims to draw independent samples from a probability distribution in order to approximate the distribution or a specific property of that distribution. (maybe an example, such as approximating pie or computing a simple 1D intergral?)

Drawing independent samples from a known distribution to then only be able to approximate said distribution appears counter-intuitive at best and wholly wasteful at worst. In practice, however, we do not actually know the distribution that we want to sample from. The most powerful and surprising insight of statistical computing, and MCMC in particular, is that we can sample from a distribution that we do not know and then use those samples to approximate the unknown distribution. We do can do this by drawing samples or "visiting" each part of the distribution in proportion to its relative probability via the use of a Markov Chain, and we can do this enough times to generate a sample that closely approximates the distribution of interest.

A Markvov Chain is a probabilistic model that describes a sequence of possible states in which the probability of each state depends only on the previous state. (include a simple diagram?) This means that no matter how the process arrived at the current state, the possible future states are fixed based on the current state. This allows you to go from one state to another repeatedly as many times as you desire or need to. The entire sequence of states you visit then represents a chain. For our purposes, we can think of states as locations in the parameter space of the distribution which we are trying to sample from, and the chain is the sample. Future states can then be determined by the relative probability density of other locations in the parameter space. This forms the basis of one of the most well-known MCMC algorithms, the Metropolis algorithm.

- metropolis algorithm
The simplest and most well known MCMC algorithm is the Metropolis algorithm. It begins by randomly selecting a starting location in the paramater space, generates a new "proposal" location to move to in the parameter space, but only moves to this new location if it has a higher density relative to the previous location or by random chance proportional to the difference in relative densities of the current location to the proposed location. As the algorithm runs for more samples, it will visit each location in the parameter space proportional to the probability density of each location, thus the sample will approximate the probability distribution more accurately as more samples are drawn. The drawback of this algorithm is trying to determine how many samples is enough. While it has been shown than the samples will tend toward the correct proportions and thus correct probability densities (cite the metro paper here), it is rather a sensitive issue to determine how many samples is enough and how accurate your approximation actually is so that you can know when to stop sampling.

There have been many advances upon and extensions of the Metropolis algorithm that attempt to improve the generalizability and efficiency of the sampling. These include but are not limited to the metropolis-hastings algorithm and Gibbs sampling (cite metropolis hastings and gibbs). These algorithms can broadly be grouped together as "guess and check" algorithms. They "guess" (often randomly) a proposal of where to move, they then "check" the posterior probability at that location and compare it to the current location. The consequence is that the quality of proposals becomes the primary bottleneck. If the algorithm makes poor proposals then much of the compute time of the algorithm is wasted when it could be touring the parameter space collecting more samples instead of rejecting proposals.

Many of the extensions of the Metropolis algorithm do try to overcome this by having a tunable step-size parameter. While it does help in some cases, this step-size parameter leads to an inescapable tradeoff between improving the acceptance rate of proposals at the cost of exploring the parameter space and vice versa. A smaller step-size will improve the acceptance rate of proposals and will lead to more samples being accepted and thus more efficient sampling, however, this comes at the cost of not being able to explore or tour the full parameter space as efficiently and thus more samples are needed to get a representative sample of the parameter space. These small steps from one proposal to the next will often result in the samples staying in the same area and often 're-exploring' the same areas opposed to exploring the full parameter space. Increasing the step-size will improve the exploration but will come at the cost of a lower acceptance rate of proposals as proposals will more often be from low probability areas of the distribution. (need to point to a figure of this, perhaps a screenshot of the animations) Furthermore, as the dimensionality of the parameter space increases so too does the concentration of measure which only exacerbates the challenge of efficiently exploring the parameter space of the typical set for these algorithms.

The fundamental issue with "guess and check" algorithms is that proposals are generally bad when they are random and don't know anything about the target distribution. This issue is further exacerbated when trying to estimate distributions that have high-dimensional parameter spaces, because of a phenomenon known as \textit{concentration of measure} (perhaps cite some of mclreaths work). Concentration of measure refers to where most of the probability density is concentrated in a distribution. This region is referred to as the \textit{typical set} and is the region we most want to sample from in order to accurately approximate the target distribution and expectations of that distribution. The issue is that the typical set in low-dimension parameter spaces, such as 1 and 2 dimensional Gaussian distributions, intuitively concentrates around the mode of the distribution. However, in higher-dimensional parameter spaces, the typical set is increasingly, and counter-intuitively, further and further away from the mode (here I should probably refer to a figure to help with clarity). This makes the random proposals from "guess and check" methods increasingly inefficient and ultimately poor estimators of distributions with many parameters. To overcome this researchers have turned to creating algorithms that try to incorporate more information from the target distribution when making proposals in order to explore the typical set more efficiently. The current most popular method is known as Hamiltonian Monte Carlo.

- hamiltonian mc
Hamiltonian Monte Carlo (HMC) exploits information about the geometry of the typical set to greatly improve the efficiency at accurately sampling the paramter space of the target distribution. The surprising insight of HMC is that for every probabilistic system there is a mathematically equivalent \textit{physical} system, with equivalent differential geometry, about which we can reason and solve for in the exact same way mathematically that a physicist would compute the conservative dynamics of a physical system via phase space and Hamilton's equations (cite beatancourt). Hence the name Hamiltonian Monte Carlo.

HMC works by exploiting the \textit{gradient} of the target probability density function, which can be used to define a vector field that we can manipulate to be aligned with the typical set. Then we can follow this vector field in order to explore and sample from the typical set more efficiently (probably need a figure). By itself, the gradient of the target probability density function points towards the mode of the distribution, and thus away from the typical set. Additional structure is required to twist the vector field generated by the gradient into a vector field corresponding to the typical set. This additional structure can be thought of as adding \textit{momentum} in such a way as to keep the corresponding dynamics of the system to be \textit{conservative}. That is to say that the conservative dynamics of the physical system requires volumes to be preserved in accordance with Hamilton's equations. A rigorous derivation and exposition of conservative dynamics and Hamilton's equations is beyond the scope of this thesis but can be found (cite some sources..). Here I will give an intuitive explanation of how conservative dynamics in physical systems works and how it relates to the probablistic systems considered in this thesis.

Intuitively, a mode, a gradient, and a typical set in a probabilistic system can be equivalently related to a planet, a gravitational field, and an orbit in a physical system (cite beatancourt now and often, or once at the end?). Exploring this physical system with a satellite is mathematically equivalent to exploring and sampling our probabilistic system. A satllite at rest will fall to the planet due to the planets gravitational pull. Adding momentum to the satellite allows it to enter a stable orbit and not be pulled into the planet. However, adding too much momentum causes the satellite the leave the stable orbit and fly out to the depths of space. Conversely, adding too little momentum causes the satellite to again be pulled into the planet. Adding just the right amount of momentum to the satelitte for it to remain in a stable orbit is the mathematical equivalent of the corresponding dynamics of the system remaining conservative, and is computed by ensuring the preservation of volume in position-momentum phase space (citation). For our purposes, this all means that the same mathematics used to compute how much momentum to add to a physical system in order to ensure the corresponding dynamics are conservative (i.e. putting a satellite into a stable orbit) can be used to twist the gradient of a target probability density function and its vector field into one that corresponds to the typical set. We can then make proposals by taking steps proportionally random to the vector field that follows the typical set. This will ensure that our sample proposals will be attracted toward the typical set, and will then stay in and efficiently explore the typical set.
