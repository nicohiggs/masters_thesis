\section{Home Advantage}
\subsection{Related Work}

The initially most well known and cited work on home advantage in sports was done in 1977 by Schwartz and Barsky \cite{Schwartz1977} who analyzed and found home advantage to exist in professional hockey, basketball, baseball and football. In \cite{Courneya1992} the authors accept home advantage as a real phenomena after reviewing the relevant literature and argue for a framework that focuses on game location, psychological states, behavioral states, and performance outcomes to try to understand the underlying causes of home advantage. Follow up work a decade later by Carron et al. \cite{Carron2005} reviewed the literature and concluded that home advantage was still present in both amateur and professional sports, in both individual and team sports, across genders, and across time. More recent works \cite{Pollard2005a} \cite{Gomez2011} confirm the continued existence of home advantage in the North American professional leagues we are considering in this study: the NHL, NBA, NFL, and MLB. In general, older studies on home advantage tend to use correlation methods of aggregated full season statistics (e.g. combining all teams home wins into one home win percentage to see if it is above 50\%), whereas more recent studies more often build statistical regression models from game level data that adjust for additional factors, such as relative team strengths, and try to infer the effect of the home advantage parameter on the regression model.

There have been several studies analyzing home advantage in the context of COVID-19 adjusted seasons; however, nearly all of them have focused exclusively on European Soccer leagues. In \cite{Benz2020} thirteen such works are summarized, of which only two used correlation methods and the other eleven made use of regression analysis to infer the change in home advantage. Benz and Lopez themselves use a bivariate Poisson regression model to infer home advantage, thus making for twelve of the fourteen studies making use of regression analysis. Ten of these studies found a drop in HA during the COVID-19 adjusted seasons, with the other four reporting mixed results where HA dropped in some leagues but not in others. We are only aware of one academic article looking at home advantage in the COVID-19 adjusted seasons for the NBA \cite{McHill2020} where the authors found presence of home advantage prior to the NBA's bubble and argue for teams travel schedules having the most notable impact. As of this writing there are no academic papers examining home advantage during the COVID-19 adjusted seasons for the NHL, NFL, or MLB, although several online blog articles exist (cite?? or leave out?) most of which take a quick cursory glance at raw home win percentages and do not account for team performance relative to strength of opponents as is done in the work summarized in \cite{Benz2020}. This paper is a first look at using regression to infer home advantage through team performance while adjusting for quality of opponents instead of only looking at aggregated statistics such as win percentage.

There is a growing body of work in sports analytics that turns to building statistical models to measure relative team strengths while accurately predicting game outcomes. These works have their roots found in Bradley-Terry models \cite{Bradley1952} and Bayesian state-space models \cite{Glickman1998}. Further advancements and examples from the NHL, NBA, NFL, and MLB are comprehensively summarized in \cite{Lopez2018} and follow a form similar to the model in \cite{Baio2010} as Bayesian methods generally offer more flexibility to be able to extend and customize these models and are generally more stable when fitting the models to data \cite{GlickmanText2017} while better capturing the uncertainty in estimating parameters opposed to classical point estimates and p-values which are increasingly under criticism in modern science. While most of this work was developed with a focus on predicting game outcomes and measuring team strengths, they often include a term to adjust for home advantage and as such can be re-purposed to be used to infer home advantage as is done in the majority of works summarized by \cite{Benz2020}. In this paper we aim to take the first attempt to use these methods to infer home advantage during the COVID-19 adjsuted seasons of the NHL, NBA, NFL, and MLB.

In \cite{Lopez2018} the authors show the improved efficacy of the Poisson distribution instead of the more common Normal distribution \cite{GlickmanText2017} for modelling points scored by each team in each game. In \cite{Benz2020} the authors follow the work in ntzoufras arguing for the use of a bivariate Poisson distribution that accounts for small correlation between two teams scoring and show its efficacy over ordinary least squares regression in inferring home advantage via simulations. However, as is shown in \cite{Baio2010} there is no need of the bivariate Poisson when working within the Bayesian framework because hierarchical models of two conditionally independent Poisson variables mix the observable variables at the upper level which results in correlations already being taken into account. In \cite{Baio2010} the authors argue for more complex methods to limit the shrinkage of their hierarchical model as their data was from leagues with a large range of team strengths. We follow \cite{Lopez2018} who showed that the "big four" North American Professional leagues are very close in team strength and thus do not reduce the shrinkage from our hierarchical model.

This next part may need to be taken out...
The challenge with methods that look at correlations among raw statistics such as home win percentage is that they fail to account for other factors such as relative team strengths. For example, a weaker team may have poor home win percentage because they have a poor overall win percentage. That same team; however, may perform better at home than they do at other stadiums whilst still losing to stronger opponents and vice versa. This discrepancy can be further impacted by imbalanced schedules. In the professional leagues we consider, teams generally do not face each each opponent the same number of times and do not face the same strength of opponents at home and away in a perfectly balanced manner. While studies often recognize this discrepancy, they often claim that it is a small effect that can be ignored \cite{Pollard2005a} without showing evidence. We argue that these issues and any debate over how much of an effect they have is most reliably mitigated by accounting for other factors, most notably team strengths, when trying to infer home advantage. Regression analysis methods are most often used for precisely their ability to account for multiple factors when performing inference, and as such they are most appropriate for our focus of analyzing home advantage.

To aid in showing the benefit of using regression to adjust for relative team strengths when trying to infer home advantage, we present a simulation of game outcomes and the difference between raw home win percentage and our models inferred results. We use a similar data generating process to \cite{Benz2020}. We generate team attacking and defensive strengths (\(\alpha_t \sim N(0, 0.5)\) and \(\delta_t \sim N(0, 0.5)\)) for \(t=1,...,20\) teams. A game is then simulated by using model (cite model number) to randomly generate the number of points scored by each team with the home advantage parameter set to \(\beta = 0.25\) and intercept \(\mu = 0\). We simulate a regular season by simulating 4 games between each pair of teams, two home games each, resulting in \(76\) game appearances for each team for a total of \(760\) games, which most closely resembles the NHL and NBA  regular season schedules. Then we simulate the top 16 teams playing a knock-out tournament bracket consisting of best 4 out of 7 matches, again closely following the NHL and NBA formats. We then compare using raw home win percentage to predict existence of home advantage compared to model (cite model). The results can be seen in figure (??).

There does exist work in baseball analytics analyzing the stability of home win percentages (cite, those articles found on your phone); however, they look at the stability over decades at a time. In our context we simply do not have decades worth of covid restricted professional games. Given the "small data" of our problem, we maintain that Bayesian inference is best suited for this task.


- high level explanation/transition to why bayesian methods are what I used

\subsection{Contribution}

We adopt a Bayesian framework to develop a Negative Binomial regression model that adjusts for relative team strengths while inferring home advantage. We choose this approach for two main reasons. First, alternative methods that rely on correlations among raw statistics, such as home win percentage, fail to account for other factors such as relative team strengths. Our regression approach can infer changes in team performance while adjusting for quality of opponents. Second, the Bayesian framework gives more interpretable results and more flexibility in model building than classical regression methods. The Bayesian framework results in distributions for the estimates of each parameter in our model. This allows us to analyze these distributions directly to determine the probability a parameter is greater (less) than a certain value or that it exists in a specific interval, avoiding the confusion that often arises interpreting p-values and confidence intervals.

By examining the resulting home advantage parameter estimates of our model from before and during the COVID-19 pandemic, we can draw conclusions about the existence of the home advantage phenomenon and provide new evidence for its potential causes. We hypothesize that home advantage is a real phenomenon, thus we expect its parameter estimate to drop during the COVID-19 seasons relative to before the COVID-19 seasons. We are also interested in examining if any differences in relative changes in home advantage exist across the leagues as some leagues had different COVID-19 restrictions which could affect home advantage differently. We also show that point totals in North American professional sports are prone to overdispersion, thus, the Negative Binomial distribution allows for better model fit than the more common Poisson and Normal distributions used in regression analyses.

\section{Bayesian Inference}

\subsection{Introduction}

- basic intro and explanation of bayes theorem
	- p(a|b) = p(b|a)p(a)/p(b) from p(a and b) <- probably should switch to theta's and x's
	- perhaps the breast cancer example
	
Bayes theorem is named such after revered thomas bayes. It was actually named posthumously after his friend found some of his work and had it published after his death. Thomas Bayes was trying to solve an inverse probability problem...

Bayes theorem can be stated simply as:
\begin{equation} \label{eq:bayes_theorem}
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
\end{equation}
This equation can be derived simply from basic rules of probability and conditional probability, namely that $P(A\cap B) = P(A|B)P(B)$ and equivalently $P(A \cap B) = P(B|A)P(A)$. One can then simply substitue and isolate $P(A|B)$ to arrive at equation \ref{eq:bayes_theorem}.

In equation \ref{eq:bayes_theorem}, each part of the equation is often referred to and interpreted differently. $P(B|A)$ is referred to as the \textit{likelihood}, and is understood in the same way likelihood is understood in traditional frequentist statistics. $P(A)$ is referred to as the \textit{prior}, and can be understood as the prior belief we have for the value of $A$. $P(B)$ is referred to as the \textit{data}...

As if often the case in science, the name of a discovery is usually not the actually discoverer or first user. In the case of Bayes theorem, it is known that (a century earlier?) Piere Simone Laplace was using what we now refer to as Bayes theorem to solve an inverse probability problem of his own. In his work...


- bayes vs frequentist
	- mention thomas bayes and laplace
	- ra fischer and the 'smear' campaign
	
Despite its early roots, bayesian statistics took a back seat during the 20th century. Most prominent statisticians of the time did not like the "subjectivity" of specifying a prior that could potentially influence the results of inference. In particular, the most prominent statistician of the 20th centry, RA Fischer, was a vocal opponent of "subjectivity". Many other prominent statisticians such as ... also comdemned the use of bayesian statistics.

The other issue with bayesian statistics that prevented it from becoming a more mainstream technique for inference is its computational difficulties. Although Bayes theorem is rather simple to state, it often leads to requiring the computation of integrals that is either exceedingly difficult or outright impossible analytically. In order to overcome this, a user is left with essentially two options to approximate the posterior distribution. The first is to change the prior and likelihood distributions into ones that are solvable analytically; this is known as choosing a conjugate prior. The issue with this method is that you are forced to use different prior and likelihood distributions than you original wanted and thus are no longer accurately representing the problem. The other method for approximation is to use a sampling procedure, such as a markov chain monte carlo (MCMC) procedure, to generate enough samples of the target posterior distribution to reasonably approximate it. The issue with this method is that MCMC sampling can be very slow taking a long time to converge and the results can be noisy and do not have any guarantees for the accuracy of the approximation.
	
- brief mention of why bayes is making a combeback now
	- real world success (mostly military)
	- advances in computing power
	- advances in mcmc theory (hmc)

Despite bayesian statistics being pushed to obscurity in the early-mid 20th century via a smear campaign and the difficulties in its computations, that latter part of the 20th century in to the 21st centruy is seeing a rise in the popularity and use of bayesian statistics through its real world successes, advances in computing power, and advances in MCMC theory.

- need real world examples from 'the theory that wouldn't die' book
- need some hard stats on advances in computing power and sampling
- need to cite beatancourt's work on HMC and future developments

\subsection{Multilevel Modeling}

MOTIVATION
- a generalization of regression methods with many use cases such as prediction, data reduction, and causal inference from experiments and observational studies.
- called multilevel or hierarchical for two reasons. 1) the structure of the data (e.g. students clustered within schools within districts within states within countries...). 2) the model itself has its own hierarchy with parameters of the within-groups regression at the bottom, controlled by the hyperparameters of the upper-level model.
- multilevel modeling can be viewed as a trade-off between two extremes: complete-pooling and no-pooling. Complete-pooling is when an overall average is used and variations among groups/categories within the overall data are ignored, thus the data are “completely pooled”. No-pooling is when separate models/averages for each individual group/category are used and any correlations or dependencies among the groups are ignored, thus the data is not “pooled” at all. In this view, multilevel models are seen as “partially-pooled” where their estimates can be thought of in a simplified way as a weighted average of the complete-pooling and no-pooling extremes. For groups/categories with fewer data points the multilevel model weighs the complete-pooling more, and for groups/categories with more data points the model weighs the no-pooling estimates more. This results in what is commonly referred to as “shrinkage” whereby partially pooled estimates are essentially the no-pooling estimates that have been “shrunk” toward to complete-pooling estimate, or “shrunk toward to the mean”. The amount of shrinkage depends on the samples-sizes, the variation within groups, and the varition between groups.
- outperforms classical regression in predictive accuracy since multilevel modeling includes least squares regression as a simple case. Generally considered essential for prediction, useful for data reduction, and helpful for causal inference.
- multilevel modeling can be viewed as a “white-box” method whereby each part of the model can be fully interpreted, understood, and customized. This makes it ideal for inference. Furthermore, multilevel models are Bayesian graphs which means that Judea Pearl’s causal calculus (or do-calculus) can be used to infer causality. This makes multilevel models useful beyond predictions alone. In contrast, this is something that many machine learning methods such as neural networks and decision trees, can not do.
- multilevel modeling allows separating estimates of predictive effects of an individual predictor and its group-level mean (usually referred to as “direct” and “contextual” effects of the predictor).
- several motivations for their use:
- learning about treatment effects that vary: how does y change when some x is varied (with all other inputs held constant)? Often its not the overall effect of x but rather how this effect varies in the population or among groups/categories of interest.
- using all the data to perform inferences for groups with small sample size: at one extreme classical estimation can be useless if the sample size is small in a group/category, and at the other a classical regression ignoring group-level variation can be misleading as well. Multilevel modeling (partial pooling) compromises between the overall noisy within-group estimates (no-pooling) and the oversimplified regression estimate that ignores group indicators (complete-pooling).
- predictions: as discussed above, the partial-pooling or “shrinkage” effect of multilevel modeling is a form of regularization that protects from underfitting (complete-pooling) and overfitting (no-pooling) to produce more accurate predictions on unseen data (the holy grail of machine learning).
- analysis of structured data and more efficient inference for regression parameters: many datasets have an inherent multilevel/hierarchical structure (e.g. students within schools, patients within hospitals, laboratory assays on plates, elections in districts within states, or data from cluster sampling etc.). Even “simple” cross-sectional data can be placed in a larger multilevel context. For example, many datasets initially thought to be “big data” often become “small data” once you being sub-dividing them into more and more groups/categories. For example, opinion polls trying to predict who you vote for based on age, race, income, location, interests etc. Each split leaves you with smaller and smaller groupings that have the potential for better model fit(more predictors) at the risk of overfitting (small samples within groups/categories).
- including predictors at two different levels: You can specify models that have individual level predictors and group level predictors. For example, in estimating radon levels in houses you could have measurements at the individual level (individual houses, indicator if the sensor is in the basement, etc.) and then predictors at the group level (county-level uranium readings) and using both together provides better model fit than separating them. Multilevel modeling avoids problems in classical regression such as colinearity when trying to include group-level indicators as well as group-level predictors in the same model.
- getting the right standard error (accurately accounting for uncertainty in prediction and estimation): To get an accurate measure of predictive uncertainty, one must account for correlation of the outcome between groups/categories/predictors (e.g. forecasting state-by-state outcomes in US election, one must account for correlation of outcome between states in a given year). Sometimes the uncertainty in estimation is of interest rather than the estimate itself. Sometimes predictions require multilevel modeling, such as when making predictions for a new group. For example, consider a model of test scores for students within schools. You could model school-level variability in classical regression (or another machine learning model such as decision trees or neural nets) with an indicator for each school. But it is impossible in this framework to make a prediction for a new student in a new school, because there is no indicator in the model for this new school. This type of problem is handled seamlessly when using the multilevel framework.

- worked out example leading to weighted average of normal dist estimates
- conjugate priors
- transition into mcmc

\subsection{Markov Chain Monte Carlo}

- basic mcmc
- advances in compute
- hamiltonian mc