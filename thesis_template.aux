\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{Permission to Use}{i}{section*.1}}
\@writefile{toc}{\contentsline {chapter}{Abstract}{iii}{section*.4}}
\@writefile{toc}{\contentsline {chapter}{Acknowledgements}{iv}{section*.6}}
\@writefile{toc}{\contentsline {chapter}{Contents}{vi}{section*.8}}
\@writefile{toc}{\contentsline {chapter}{List of Tables}{vii}{section*.9}}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{viii}{section*.10}}
\gdef \LT@i {\LT@entry 
    {1}{46.8612pt}\LT@entry 
    {1}{425.17003pt}}
\@writefile{toc}{\contentsline {chapter}{List of Abbreviations}{ix}{section*.11}}
\citation{Schwartz1977}
\citation{Courneya1992}
\citation{Nevill1999}
\citation{Agnew1994}
\citation{Unkelbach2010}
\citation{Forrest2005}
\citation{Dohmen2016}
\citation{Buraimo2010}
\citation{Lopez2018}
\citation{Benz2020}
\citation{Benz2020}
\citation{McHill2020}
\citation{nhl2020}
\citation{usatoday2020}
\citation{Unkelbach2010}
\citation{Buraimo2010}
\citation{Courneya1992}
\citation{Carron2005}
\citation{McHill2020}
\citation{Garicano2005}
\citation{Moskowitz2012}
\citation{McHill2020}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.0.1}Contribution}{2}{subsection.1.0.1}}
\citation{Schwartz1977}
\citation{Courneya1992}
\citation{Carron2005}
\citation{Pollard2005a}
\citation{Gomez2011}
\citation{Benz2020}
\citation{McHill2020}
\citation{Benz2020}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{4}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Home Advantage}{4}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Related Work}{4}{subsection.2.1.1}}
\citation{Bradley1952}
\citation{Glickman1998}
\citation{Lopez2018}
\citation{Baio2010}
\citation{GlickmanText2017}
\citation{Benz2020}
\citation{Lopez2018}
\citation{GlickmanText2017}
\citation{Benz2020}
\citation{Baio2010}
\citation{Baio2010}
\citation{Lopez2018}
\citation{Pollard2005a}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Bayesian Inference}{6}{section.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Introduction}{6}{subsection.2.2.1}}
\newlabel{eq:orig_bayes_theorem}{{2.1}{6}{Introduction}{equation.2.2.1}{}}
\newlabel{eq:new_bayes_theorem}{{2.2}{7}{Introduction}{equation.2.2.2}{}}
\newlabel{eq:proportional_bayes_theorem}{{2.3}{7}{Introduction}{equation.2.2.3}{}}
\newlabel{eq:normalization_factor}{{2.4}{8}{Introduction}{equation.2.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Multilevel Modeling}{8}{subsection.2.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Markov Chain Monte Carlo}{11}{subsection.2.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Hamiltonian Monte Carlo}{14}{subsection.2.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Model Evaluation and Selection}{15}{subsection.2.2.5}}
\@writefile{toc}{\contentsline {subsubsection}{Cross Validation, Information Criterion, and Beyond}{15}{subsubsection*.13}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Example of how increasing model complexity leads to better model fit on the train-set, but can come at the cost of increasingly worse performance on the test-set. Model fit here is measured visually and in terms of mean-squared-error (MSE) and R squared ($R^2$). The dataset in (a) generated by a degree-2 polynomial with some added noise is split into train and test sets. A degree-1 polynomial underfits the data (b). A more complex degree-2 polynomial improves the fit (c). A far more complex degree-16 polynomial fits the train set extremely well but is overfit as evidenced by its poor test set performance (d).\relax }}{16}{figure.caption.14}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:overfitting_example}{{2.1}{16}{Example of how increasing model complexity leads to better model fit on the train-set, but can come at the cost of increasingly worse performance on the test-set. Model fit here is measured visually and in terms of mean-squared-error (MSE) and R squared ($R^2$). The dataset in (a) generated by a degree-2 polynomial with some added noise is split into train and test sets. A degree-1 polynomial underfits the data (b). A more complex degree-2 polynomial improves the fit (c). A far more complex degree-16 polynomial fits the train set extremely well but is overfit as evidenced by its poor test set performance (d).\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{16}{subfigure.1.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{16}{subfigure.1.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{16}{subfigure.1.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{16}{subfigure.1.4}}
\newlabel{eq:lppd}{{2.9}{18}{Cross Validation, Information Criterion, and Beyond}{equation.2.2.9}{}}
\newlabel{eq:waic}{{2.11}{19}{Cross Validation, Information Criterion, and Beyond}{equation.2.2.11}{}}
\newlabel{eq:psis-loo}{{2.12}{20}{Cross Validation, Information Criterion, and Beyond}{equation.2.2.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{Posterior Predictive Checks}{21}{subsubsection*.15}}
\citation{Baio2010}
\citation{Glickman1998}
\citation{Lopez2018}
\citation{Benz2020}
\citation{Gelman2014}
\citation{Gelman2006}
\citation{McElreath2020}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methods}{22}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Multilevel Model}{22}{section.3.1}}
\newlabel{multilevel_model}{{3.1}{22}{Multilevel Model}{section.3.1}{}}
\citation{McElreath2020}
\citation{Gelman2014}
\citation{Gelman2006}
\citation{Baio2010}
\newlabel{eq:expected points}{{3.1}{23}{Multilevel Model}{equation.3.1.1}{}}
\newlabel{eq:sum to zero}{{3.2}{23}{Multilevel Model}{equation.3.1.2}{}}
\citation{pymc3}
\citation{Baio2010}
\citation{Benz2020}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Model Fit in PyMC3}{24}{subsection.3.1.1}}
\citation{Gelman1992}
\citation{Brooks1997}
\citation{Betancourt2017}
\newlabel{eq:priors}{{3.4}{25}{Model Fit in PyMC3}{equation.3.1.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Data Experiments}{25}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Complete pooling, No pooling, and Partial pooling}{25}{subsection.3.2.1}}
\newlabel{eq:cp_model}{{3.5}{25}{Complete pooling, No pooling, and Partial pooling}{equation.3.2.5}{}}
\citation{Karlis2003}
\citation{Baio2010}
\citation{Benz2020}
\citation{Payne2018}
\citation{Cameron1990}
\citation{Vehtari2016}
\citation{Watanabe2010}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Negative Binomial Regression}{27}{subsection.3.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Inferring Home Advantage}{27}{subsection.3.2.3}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Results}{29}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Complete pooling, No pooling, Partial pooling}{29}{section.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Comparison of models via their Log-Score on train and test sets, as well as the PSIS-LOO estimated Log-Score, for each league. The complete-pooling model underfits, the no-pooling model overfits, and the partial-pooling model provides the best tradeoff in fitting the data while protecting against overfitting. The PSIS-LOO estimates consistently predict how the models would rank on an unseen test-set.\relax }}{30}{figure.caption.16}}
\newlabel{fig:log_scores}{{4.1}{30}{Comparison of models via their Log-Score on train and test sets, as well as the PSIS-LOO estimated Log-Score, for each league. The complete-pooling model underfits, the no-pooling model overfits, and the partial-pooling model provides the best tradeoff in fitting the data while protecting against overfitting. The PSIS-LOO estimates consistently predict how the models would rank on an unseen test-set.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {NHL}}}{30}{subfigure.1.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {NBA}}}{30}{subfigure.1.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {MLB}}}{30}{subfigure.1.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {NFL}}}{30}{subfigure.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Comparison of models via their PSIS-LOO estimated Log-Score for each league, ranked from best (highest) to worst (lowest) on the y-axis. The black points and lines represent the point estimate and its standard error. The grey triangle and lines represent the estimated difference and the standard error of the difference for each model relative to the best model. The standard error of the difference is generally much smaller than the standard error of the estimate because errors in the estimates for each model are highly correlated.\relax }}{31}{figure.caption.17}}
\newlabel{fig:psis_loo}{{4.2}{31}{Comparison of models via their PSIS-LOO estimated Log-Score for each league, ranked from best (highest) to worst (lowest) on the y-axis. The black points and lines represent the point estimate and its standard error. The grey triangle and lines represent the estimated difference and the standard error of the difference for each model relative to the best model. The standard error of the difference is generally much smaller than the standard error of the estimate because errors in the estimates for each model are highly correlated.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {NHL}}}{31}{subfigure.2.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {NBA}}}{31}{subfigure.2.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {MLB}}}{31}{subfigure.2.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {NFL}}}{31}{subfigure.2.4}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Comparison of estimated negative log-likelihood of leave-one-out cross-validation (LOO) for each model across each league. The differences between the Poisson, Negative Binomial (NB), and Normal models are reported relative to the best fitting model (dLOO) for each league; along with the standard error of the estimated differences (dSE). The dispersion statistic, \(\sigma _p\), indicates how much greater the variance is than the mean for point totals in each league and signals overdispersion when \(\sigma _p > 2\). The NB model noticeably outperforms the Poisson model for leagues with greater overdispersion (MLB and NFL) while being nearly identical for leagues with little to no overdispersion (NHL and NBA). The NB model also outperforms the Normal model in each league except the NFL where they are close to one another while both vastly outperforming the Poisson model.\relax }}{32}{table.caption.19}}
\newlabel{tab:loo}{{4.1}{32}{Comparison of estimated negative log-likelihood of leave-one-out cross-validation (LOO) for each model across each league. The differences between the Poisson, Negative Binomial (NB), and Normal models are reported relative to the best fitting model (dLOO) for each league; along with the standard error of the estimated differences (dSE). The dispersion statistic, \(\sigma _p\), indicates how much greater the variance is than the mean for point totals in each league and signals overdispersion when \(\sigma _p > 2\). The NB model noticeably outperforms the Poisson model for leagues with greater overdispersion (MLB and NFL) while being nearly identical for leagues with little to no overdispersion (NHL and NBA). The NB model also outperforms the Normal model in each league except the NFL where they are close to one another while both vastly outperforming the Poisson model.\relax }{table.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Negative Binomial Regression}{32}{subsection.4.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Comparison of distribution of home points in the models and the observed data for each league. The Negative Binomial model noticeably provides a better overall fit across each league.\relax }}{33}{figure.caption.18}}
\newlabel{fig:comparisons}{{4.3}{33}{Comparison of distribution of home points in the models and the observed data for each league. The Negative Binomial model noticeably provides a better overall fit across each league.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Distributions of the estimated home advantage for the NHL, NBA, MLB, and NFL for pre and post COVID adjusted seasons. Home advantage for playoffs are reported for NHL and NBA because that is when their COVID restricted games took place. Home advantage for regular season is reported for MLB and NFL as their respective playoff seasons are too small for stable results. Red distributions represent COVID-19 bubble adjusted seasons.\relax }}{34}{figure.caption.20}}
\newlabel{fig:ha_pooled}{{4.4}{34}{Distributions of the estimated home advantage for the NHL, NBA, MLB, and NFL for pre and post COVID adjusted seasons. Home advantage for playoffs are reported for NHL and NBA because that is when their COVID restricted games took place. Home advantage for regular season is reported for MLB and NFL as their respective playoff seasons are too small for stable results. Red distributions represent COVID-19 bubble adjusted seasons.\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Inferring Home Advantage}{34}{subsection.4.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Distributions of the estimated home advantage for the NHL, NBA, MLB, and NFL over the past 5 seasons from 2016-2020. Home advantage for playoffs are reported for NHL and NBA because that is when their COVID restricted games took place. Home advantage for regular season is reported for MLB and NFL as their respective playoff seasons are too small for stable results. Red distributions represent COVID-19 bubble adjusted seasons.\relax }}{37}{figure.caption.21}}
\newlabel{fig:ha_main}{{4.5}{37}{Distributions of the estimated home advantage for the NHL, NBA, MLB, and NFL over the past 5 seasons from 2016-2020. Home advantage for playoffs are reported for NHL and NBA because that is when their COVID restricted games took place. Home advantage for regular season is reported for MLB and NFL as their respective playoff seasons are too small for stable results. Red distributions represent COVID-19 bubble adjusted seasons.\relax }{figure.caption.21}{}}
\citation{McHill2020}
\citation{Hall2020}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusions}{38}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{Pollard2005a}
\citation{pymc3}
\citation{Betancourt2017}
\citation{Lopez2018}
\citation{Glickman1998}
\citation{Karlis2003}
\citation{Baio2010}
\citation{Benz2020}
\citation{Gelman2006}
\citation{Gelman2014}
\citation{McElreath2020}
\citation{Benz2020}
\citation{Unkelbach2010}
\citation{Buraimo2010}
\citation{Dohmen2016}
\bibstyle{plain}
\bibdata{880Paper}
\@writefile{toc}{\contentsline {chapter}{References}{41}{section*.22}}
\@writefile{toc}{\contentsline {chapter}{Appendix \numberline {A}Sample Appendix}{41}{Appendix.1.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{Appendix \numberline {B}Another Sample Appendix}{42}{Appendix.1.B}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
