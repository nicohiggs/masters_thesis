\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{Permission to Use}{i}{section*.1}}
\@writefile{toc}{\contentsline {chapter}{Abstract}{iii}{section*.4}}
\@writefile{toc}{\contentsline {chapter}{Acknowledgements}{iv}{section*.6}}
\@writefile{toc}{\contentsline {chapter}{Contents}{vi}{section*.8}}
\@writefile{toc}{\contentsline {chapter}{List of Tables}{vii}{section*.9}}
\citation{Betancourt2017}
\citation{Betancourt2017}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{viii}{section*.10}}
\gdef \LT@i {\LT@entry 
    {1}{74.94456pt}\LT@entry 
    {1}{397.08667pt}}
\@writefile{toc}{\contentsline {chapter}{List of Abbreviations}{x}{section*.11}}
\citation{Higgs2021}
\citation{Higgs2021}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Co-Authorship Statement}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{Schwartz1977}
\citation{Courneya1992}
\citation{Nevill1999}
\citation{Agnew1994}
\citation{Unkelbach2010}
\citation{Forrest2005}
\citation{Dohmen2016}
\citation{Buraimo2010}
\citation{Lopez2018}
\citation{Benz2020}
\citation{Benz2020}
\citation{McHill2020}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Introduction}{2}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{nhl2020}
\citation{usatoday2020}
\citation{Unkelbach2010}
\citation{Buraimo2010}
\citation{Courneya1992}
\citation{Carron2005}
\citation{McHill2020}
\citation{Garicano2005}
\citation{Moskowitz2012}
\citation{McHill2020}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Contribution}{3}{section.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Thesis Organization}{5}{section.2.2}}
\citation{David1998}
\citation{Moivre1718}
\citation{Stigler1986}
\citation{Schoot2021}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Background on Bayesian Inference}{6}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{6}{section.3.1}}
\newlabel{eq:orig_bayes_theorem}{{3.1}{6}{Introduction}{equation.3.1.1}{}}
\citation{Schoot2021}
\citation{McGrayne2011}
\citation{Ioannidis2005}
\citation{Begley2015}
\citation{Box1976}
\newlabel{eq:new_bayes_theorem}{{3.2}{7}{Introduction}{equation.3.1.2}{}}
\newlabel{eq:proportional_bayes_theorem}{{3.3}{7}{Introduction}{equation.3.1.3}{}}
\citation{Schoot2021}
\newlabel{eq:normalization_factor}{{3.4}{8}{}{equation.3.1.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Multilevel Modelling}{8}{section.3.2}}
\newlabel{Multilevel_Modelling}{{3.2}{8}{Multilevel Modelling}{section.3.2}{}}
\newlabel{eq:mlm_ex}{{3.5}{9}{Multilevel Modelling}{equation.3.2.5}{}}
\citation{Gelman2006}
\citation{Gelman2006}
\citation{Gelman2006b}
\citation{pymc32018}
\citation{Gelman2006b}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Comparison of county parameter estimates between traditional regression (no pooling) and multilevel regression (partial pooling). Notice how the partial pooling estimates ``shrink toward the mean''. Further note how this shrinkage is greater for the counties with fewer observations and lesser for counties with more observations.\relax }}{10}{figure.caption.13}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:radon_example}{{3.1}{10}{Comparison of county parameter estimates between traditional regression (no pooling) and multilevel regression (partial pooling). Notice how the partial pooling estimates ``shrink toward the mean''. Further note how this shrinkage is greater for the counties with fewer observations and lesser for counties with more observations.\relax }{figure.caption.13}{}}
\citation{Gelman2006b}
\citation{Gelman2014}
\citation{Gelman2006}
\citation{McElreath2020}
\citation{Pearl2000}
\citation{Gelman2006b}
\citation{Fox2008}
\citation{Betancourt2017}
\citation{Betancourt2017}
\citation{Bishop2006}
\citation{Betancourt2017}
\citation{Carpenter2017}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Markov Chain Monte Carlo}{13}{section.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Under ideal circumstances a Markov chain will first converge to the typical set (a) and then explore it efficiently (b). Unfortunately, in higher dimensions most MCMC algorithms struggle to explore the typical set and inefficiently sample a small portion (c, green). We desire algorithms that make use of the geometry of the target distribution to properly explore the typical set during sampling (d). Images are from Figures 7, 10, 11 of \cite  {Betancourt2017}. Permission to use was granted by the author under a CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/).\relax }}{14}{figure.caption.14}}
\newlabel{fig:mcmc}{{3.2}{14}{Under ideal circumstances a Markov chain will first converge to the typical set (a) and then explore it efficiently (b). Unfortunately, in higher dimensions most MCMC algorithms struggle to explore the typical set and inefficiently sample a small portion (c, green). We desire algorithms that make use of the geometry of the target distribution to properly explore the typical set during sampling (d). Images are from Figures 7, 10, 11 of \cite {Betancourt2017}. Permission to use was granted by the author under a CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/).\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{14}{subfigure.2.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{14}{subfigure.2.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{14}{subfigure.2.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{14}{subfigure.2.4}}
\citation{Metropolis1953}
\citation{Hastings1970}
\citation{Hastings1970}
\citation{German1984}
\citation{Betancourt2017}
\citation{Carpenter2017}
\citation{Betancourt2017}
\citation{Betancourt2017}
\citation{Betancourt2017}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The gradient and corresponding vector field of a probability distribution points to its mode which is often away from the typical set in higher dimensions (a). Ideally we want to twist the vector field to align with the typical set (b). The mode, gradient, and typical set of a probabilistic system are mathematically equivalent to a planet, gravitational field, and orbit in a physical system (c). Adding momentum to the system to cause a satellite to enter a stable orbit (d) is equivalent to twisting a vector field to align with the typical set of a probabilistic system. Images are from Figures 12, 13, 14, 17 of \cite  {Betancourt2017}. Permission to use was granted by the author under a CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/).\relax }}{17}{figure.caption.15}}
\newlabel{fig:hmc}{{3.3}{17}{The gradient and corresponding vector field of a probability distribution points to its mode which is often away from the typical set in higher dimensions (a). Ideally we want to twist the vector field to align with the typical set (b). The mode, gradient, and typical set of a probabilistic system are mathematically equivalent to a planet, gravitational field, and orbit in a physical system (c). Adding momentum to the system to cause a satellite to enter a stable orbit (d) is equivalent to twisting a vector field to align with the typical set of a probabilistic system. Images are from Figures 12, 13, 14, 17 of \cite {Betancourt2017}. Permission to use was granted by the author under a CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/).\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{17}{subfigure.3.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{17}{subfigure.3.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{17}{subfigure.3.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{17}{subfigure.3.4}}
\citation{Betancourt2017}
\citation{Betancourt2017}
\citation{pymc3}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Model Evaluation and Selection}{19}{section.3.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Example of how increasing model complexity leads to better model fit on the train-set, but can come at the cost of increasingly worse performance on the test-set. Model fit here is measured visually and in terms of mean-squared-error (MSE: lower is better) and R-squared ($R^2$: closer to 1 is better). The dataset in (a) is generated by a degree-2 polynomial with some added noise and is split into train and test sets. A degree-1 polynomial underfits the data (b). More complex polynomials improve the fit on the train-set (c, d, e). However, increasingly complex polynomials become overfit as evidenced by increasingly worse test-set performance (d, e). The overall trend of increasing model complexity, how it relates to underfitting and overfitting, and where the tradeoff is optimal is captured in (f).\relax }}{20}{figure.caption.16}}
\newlabel{fig:overfitting_example}{{3.4}{20}{Example of how increasing model complexity leads to better model fit on the train-set, but can come at the cost of increasingly worse performance on the test-set. Model fit here is measured visually and in terms of mean-squared-error (MSE: lower is better) and R-squared ($R^2$: closer to 1 is better). The dataset in (a) is generated by a degree-2 polynomial with some added noise and is split into train and test sets. A degree-1 polynomial underfits the data (b). More complex polynomials improve the fit on the train-set (c, d, e). However, increasingly complex polynomials become overfit as evidenced by increasingly worse test-set performance (d, e). The overall trend of increasing model complexity, how it relates to underfitting and overfitting, and where the tradeoff is optimal is captured in (f).\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{20}{subfigure.4.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{20}{subfigure.4.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{20}{subfigure.4.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{20}{subfigure.4.4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {}}}{20}{subfigure.4.5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {}}}{20}{subfigure.4.6}}
\citation{Murphy2012}
\citation{Shannon1948}
\citation{McElreath2020}
\citation{McElreath2020}
\citation{McElreath2020}
\citation{McElreath2020}
\citation{Dunn2018}
\citation{Akaike1974}
\newlabel{eq:log-score}{{3.8}{23}{}{equation.3.4.8}{}}
\newlabel{eq:lppd}{{3.9}{23}{}{equation.3.4.9}{}}
\citation{Watanabe2010}
\citation{Vehtari2016}
\newlabel{eq:waic}{{3.11}{24}{}{equation.3.4.11}{}}
\citation{Vehtari2016}
\newlabel{eq:psis-loo}{{3.12}{25}{}{equation.3.4.12}{}}
\newlabel{ppc}{{3.4}{25}{}{equation.3.4.12}{}}
\citation{Schwartz1977}
\citation{Courneya1992}
\citation{Carron2005}
\citation{Pollard2005a}
\citation{Gomez2011}
\citation{Benz2020}
\citation{McHill2020}
\citation{Bradley1952}
\citation{Glickman1998}
\citation{Lopez2018}
\citation{Baio2010}
\citation{GlickmanText2017}
\citation{Ioannidis2005}
\citation{Begley2015}
\citation{Benz2020}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Related Work on Sports Analytics}{26}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{Lopez2018}
\citation{GlickmanText2017}
\citation{Benz2020}
\citation{Karlis2003}
\citation{Baio2010}
\citation{Baio2010}
\citation{Lopez2018}
\citation{Pollard2005a}
\citation{Baio2010}
\citation{Glickman1998}
\citation{Lopez2018}
\citation{Benz2020}
\citation{Gelman2014}
\citation{Gelman2006}
\citation{McElreath2020}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Methods}{28}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Multilevel Model}{28}{section.5.1}}
\newlabel{multilevel_model}{{5.1}{28}{Multilevel Model}{section.5.1}{}}
\citation{McElreath2020}
\citation{Gelman2014}
\citation{Gelman2006}
\citation{Baio2010}
\citation{Baio2010}
\citation{Benz2020}
\citation{Karlis2003}
\citation{McElreath2020}
\citation{Baio2010}
\citation{Benz2020}
\citation{Karlis2003}
\newlabel{eq:likelihood}{{5.1}{29}{Multilevel Model}{equation.5.1.1}{}}
\newlabel{eq:expected points}{{5.2}{29}{Multilevel Model}{equation.5.1.2}{}}
\newlabel{eq:sum to zero}{{5.3}{29}{Multilevel Model}{equation.5.1.3}{}}
\citation{pymc3}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Model Fit in PyMC3}{30}{subsection.5.1.1}}
\newlabel{pymc3}{{5.1.1}{30}{Model Fit in PyMC3}{subsection.5.1.1}{}}
\citation{Baio2010}
\citation{Benz2020}
\citation{pymc3}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces An example of how a Bayesian model would be defined in an academic textbook or paper (a) and how a probabilistic programming language such as PyMC3 would create the model in Python code (b). Note how the priors have to be defined first because the code will be executed procedurally. The two definitions are essentially identical otherwise.\relax }}{31}{figure.caption.17}}
\newlabel{fig:pymc3_code}{{5.1}{31}{An example of how a Bayesian model would be defined in an academic textbook or paper (a) and how a probabilistic programming language such as PyMC3 would create the model in Python code (b). Note how the priors have to be defined first because the code will be executed procedurally. The two definitions are essentially identical otherwise.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{31}{subfigure.1.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{31}{subfigure.1.2}}
\newlabel{eq:priors}{{5.5}{31}{Model Fit in PyMC3}{equation.5.1.5}{}}
\citation{Gelman1992}
\citation{Brooks1997}
\citation{Betancourt2017}
\citation{NS2020}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Experiments}{32}{section.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Data}{32}{subsection.5.2.1}}
\citation{BR2020}
\citation{RS2020}
\citation{FR2020}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Complete pooling, No pooling, and Partial pooling}{34}{subsection.5.2.2}}
\newlabel{experiment_1}{{5.2.2}{34}{Complete pooling, No pooling, and Partial pooling}{subsection.5.2.2}{}}
\newlabel{eq:cp_model}{{5.6}{34}{}{equation.5.2.6}{}}
\citation{Karlis2003}
\citation{Baio2010}
\citation{Benz2020}
\citation{Payne2018}
\citation{Cameron1990}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Negative Binomial Regression}{35}{subsection.5.2.3}}
\newlabel{experiment_2}{{5.2.3}{35}{Negative Binomial Regression}{subsection.5.2.3}{}}
\citation{Vehtari2016}
\citation{Watanabe2010}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.4}Inferring Home Advantage}{36}{subsection.5.2.4}}
\newlabel{experiment_3}{{5.2.4}{36}{Inferring Home Advantage}{subsection.5.2.4}{}}
\citation{Vehtari2016}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Results}{37}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Complete pooling, No pooling, Partial pooling}{37}{section.6.1}}
\newlabel{results_1}{{6.1}{37}{Complete pooling, No pooling, Partial pooling}{section.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Comparison of models via their Log-Score (higher is better) on train and test sets, as well as the PSIS-LOO estimated Log-Score, for each league. The complete-pooling model under-fits, the no-pooling model over-fits, and the partial-pooling model provides the best trade-off in fitting the data while protecting against over-fitting. The PSIS-LOO estimates consistently predict how the models would rank on an unseen test-set.\relax }}{38}{figure.caption.18}}
\newlabel{fig:log_scores}{{6.1}{38}{Comparison of models via their Log-Score (higher is better) on train and test sets, as well as the PSIS-LOO estimated Log-Score, for each league. The complete-pooling model under-fits, the no-pooling model over-fits, and the partial-pooling model provides the best trade-off in fitting the data while protecting against over-fitting. The PSIS-LOO estimates consistently predict how the models would rank on an unseen test-set.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {NHL}}}{38}{subfigure.1.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {NBA}}}{38}{subfigure.1.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {MLB}}}{38}{subfigure.1.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {NFL}}}{38}{subfigure.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Comparison of models via their PSIS-LOO estimated Log-Score for each league, ranked from best (highest) to worst (lowest) on the y-axis. The black points and lines represent the point estimate and its standard error. The grey triangle and lines represent the estimated difference and the standard error of the difference for each model relative to the best model. The standard error of the difference is generally much smaller than the standard error of the estimate because errors in the estimates for each model are highly correlated.\relax }}{39}{figure.caption.19}}
\newlabel{fig:psis_loo}{{6.2}{39}{Comparison of models via their PSIS-LOO estimated Log-Score for each league, ranked from best (highest) to worst (lowest) on the y-axis. The black points and lines represent the point estimate and its standard error. The grey triangle and lines represent the estimated difference and the standard error of the difference for each model relative to the best model. The standard error of the difference is generally much smaller than the standard error of the estimate because errors in the estimates for each model are highly correlated.\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {NHL}}}{39}{subfigure.2.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {NBA}}}{39}{subfigure.2.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {MLB}}}{39}{subfigure.2.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {NFL}}}{39}{subfigure.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Negative Binomial Regression}{40}{section.6.2}}
\newlabel{results_2}{{6.2}{40}{Negative Binomial Regression}{section.6.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Inferring Home Advantage}{40}{section.6.3}}
\newlabel{results_3}{{6.3}{40}{Inferring Home Advantage}{section.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Comparison of distribution of home points in the models and the observed data for each league. The Negative Binomial model noticeably provides a better overall fit across each league.\relax }}{41}{figure.caption.20}}
\newlabel{fig:comparisons}{{6.3}{41}{Comparison of distribution of home points in the models and the observed data for each league. The Negative Binomial model noticeably provides a better overall fit across each league.\relax }{figure.caption.20}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Comparison of estimated negative log-likelihood of leave-one-out cross-validation (LOO) for each model across each league. The differences between the Poisson, Negative Binomial (NB), and Normal models are reported relative to the best fitting model (dLOO) for each league; along with the standard error of the estimated differences (dSE). The dispersion statistic, \(\sigma _p\), indicates how much greater the variance is than the mean for point totals in each league and signals overdispersion when \(\sigma _p > 2\). The NB model noticeably outperforms the Poisson model for leagues with greater overdispersion (MLB and NFL) while being nearly identical for leagues with little to no overdispersion (NHL and NBA). The NB model also outperforms the Normal model in each league except the NFL where they are close to one another while both vastly outperforming the Poisson model.\relax }}{42}{table.caption.21}}
\newlabel{tab:loo}{{6.1}{42}{Comparison of estimated negative log-likelihood of leave-one-out cross-validation (LOO) for each model across each league. The differences between the Poisson, Negative Binomial (NB), and Normal models are reported relative to the best fitting model (dLOO) for each league; along with the standard error of the estimated differences (dSE). The dispersion statistic, \(\sigma _p\), indicates how much greater the variance is than the mean for point totals in each league and signals overdispersion when \(\sigma _p > 2\). The NB model noticeably outperforms the Poisson model for leagues with greater overdispersion (MLB and NFL) while being nearly identical for leagues with little to no overdispersion (NHL and NBA). The NB model also outperforms the Normal model in each league except the NFL where they are close to one another while both vastly outperforming the Poisson model.\relax }{table.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Distributions of the estimated home advantage for the NHL, NBA, MLB, and NFL for pre and post COVID adjusted seasons. Home advantage for playoffs are reported for NHL and NBA because that is when their COVID restricted games took place. Home advantage for regular season is reported for MLB and NFL as their respective playoff seasons are too small for stable results. Red distributions represent COVID-19 bubble adjusted seasons.\relax }}{43}{figure.caption.22}}
\newlabel{fig:ha_pooled}{{6.4}{43}{Distributions of the estimated home advantage for the NHL, NBA, MLB, and NFL for pre and post COVID adjusted seasons. Home advantage for playoffs are reported for NHL and NBA because that is when their COVID restricted games took place. Home advantage for regular season is reported for MLB and NFL as their respective playoff seasons are too small for stable results. Red distributions represent COVID-19 bubble adjusted seasons.\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Distributions of the estimated home advantage for the NHL, NBA, MLB, and NFL over the past 5 seasons from 2016-2020. Home advantage for playoffs are reported for NHL and NBA because that is when their COVID restricted games took place. Home advantage for regular season is reported for MLB and NFL as their respective playoff seasons are too small for stable results. Red distributions represent COVID-19 bubble adjusted seasons.\relax }}{45}{figure.caption.23}}
\newlabel{fig:ha_main}{{6.5}{45}{Distributions of the estimated home advantage for the NHL, NBA, MLB, and NFL over the past 5 seasons from 2016-2020. Home advantage for playoffs are reported for NHL and NBA because that is when their COVID restricted games took place. Home advantage for regular season is reported for MLB and NFL as their respective playoff seasons are too small for stable results. Red distributions represent COVID-19 bubble adjusted seasons.\relax }{figure.caption.23}{}}
\citation{McHill2020}
\citation{Hall2020}
\citation{Pollard2005a}
\citation{pymc3}
\citation{Betancourt2017}
\citation{Lopez2018}
\citation{Glickman1998}
\citation{Karlis2003}
\citation{Baio2010}
\citation{Benz2020}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Discussion and Conclusions}{46}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Discussion}{46}{section.7.1}}
\citation{Gelman2006}
\citation{Gelman2014}
\citation{McElreath2020}
\citation{Benz2020}
\citation{Unkelbach2010}
\citation{Buraimo2010}
\citation{Dohmen2016}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Limitations}{48}{section.7.2}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Conclusions}{48}{section.7.3}}
\bibstyle{plain}
\bibdata{habib}
\bibcite{Agnew1994}{1}
\bibcite{Akaike1974}{2}
\bibcite{GlickmanText2017}{3}
\bibcite{Baio2010}{4}
\bibcite{Begley2015}{5}
\bibcite{Benz2020}{6}
\bibcite{Betancourt2017}{7}
\bibcite{Bishop2006}{8}
\bibcite{Box1976}{9}
\bibcite{Bradley1952}{10}
\bibcite{Brooks1997}{11}
\bibcite{Buraimo2010}{12}
\bibcite{Cameron1990}{13}
\bibcite{Carpenter2017}{14}
\bibcite{Carron2005}{15}
\bibcite{nhl2020}{16}
\bibcite{Courneya1992}{17}
\bibcite{David1998}{18}
\bibcite{Dohmen2016}{19}
\bibcite{Dunn2018}{20}
\@writefile{toc}{\contentsline {chapter}{References}{50}{section*.24}}
\bibcite{Forrest2005}{21}
\bibcite{Fox2008}{22}
\bibcite{Garicano2005}{23}
\bibcite{Gelman2006b}{24}
\bibcite{Gelman1992}{25}
\bibcite{Gelman2014}{26}
\bibcite{Gelman2006}{27}
\bibcite{German1984}{28}
\bibcite{Glickman1998}{29}
\bibcite{Gomez2011}{30}
\bibcite{Hall2020}{31}
\bibcite{Hastings1970}{32}
\bibcite{Higgs2021}{33}
\bibcite{Ioannidis2005}{34}
\bibcite{Karlis2003}{35}
\bibcite{Lopez2018}{36}
\bibcite{McElreath2020}{37}
\bibcite{McGrayne2011}{38}
\bibcite{McHill2020}{39}
\bibcite{Metropolis1953}{40}
\bibcite{Moivre1718}{41}
\bibcite{Moskowitz2012}{42}
\bibcite{Murphy2012}{43}
\bibcite{NS2020}{44}
\bibcite{Nevill1999}{45}
\bibcite{Payne2018}{46}
\bibcite{Pearl2000}{47}
\bibcite{Pollard2005a}{48}
\bibcite{RS2020}{49}
\bibcite{pymc3}{50}
\bibcite{Schoot2021}{51}
\bibcite{Schwartz1977}{52}
\bibcite{Shannon1948}{53}
\bibcite{BR2020}{54}
\bibcite{FR2020}{55}
\bibcite{Stigler1986}{56}
\bibcite{pymc32018}{57}
\bibcite{Unkelbach2010}{58}
\bibcite{Vehtari2016}{59}
\bibcite{Watanabe2010}{60}
\bibcite{usatoday2020}{61}
\@writefile{toc}{\contentsline {chapter}{Appendix \numberline {A}Appendix}{53}{Appendix.1.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces The estimated probabilities that the home advantage parameter during the 2020 COVID-19 restricted games ($\beta _{20}$) is less than 0, the previous four seasons (2016-2019) mean ($\mathaccentV {bar}016{\beta }_{16-19}$), and the previous seasons individual means ($\mathaccentV {bar}016{\beta }_{19}$, $\mathaccentV {bar}016{\beta }_{18}$, $\mathaccentV {bar}016{\beta }_{17}$, $\mathaccentV {bar}016{\beta }_{16}$).\relax }}{53}{table.caption.26}}
\newlabel{tab:ha_probs}{{A.1}{53}{The estimated probabilities that the home advantage parameter during the 2020 COVID-19 restricted games ($\beta _{20}$) is less than 0, the previous four seasons (2016-2019) mean ($\bar {\beta }_{16-19}$), and the previous seasons individual means ($\bar {\beta }_{19}$, $\bar {\beta }_{18}$, $\bar {\beta }_{17}$, $\bar {\beta }_{16}$).\relax }{table.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces Offensive and Defensive team ratings for the 2020 NHL season. The points with the team labels next to them are ratings generated by traditional regression, and the corresponding ratings are generated from multilevel regression to highlight the effect of shrinkage to the mean.\relax }}{54}{figure.caption.27}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces Offensive and Defensive team ratings for the 2020 NHL season. The points with the team labels next to them are ratings generated by traditional regression, and the corresponding ratings are generated from multilevel regression to highlight the effect of shrinkage to the mean.\relax }}{55}{figure.caption.28}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.3}{\ignorespaces Offensive and Defensive team ratings for the 2020 MLB season. The points with the team labels next to them are ratings generated by traditional regression, and the corresponding ratings are generated from multilevel regression to highlight the effect of shrinkage to the mean.\relax }}{56}{figure.caption.29}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.4}{\ignorespaces Offensive and Defensive team ratings for the 2020 NFL season. The points with the team labels next to them are ratings generated by traditional regression, and the corresponding ratings are generated from multilevel regression to highlight the effect of shrinkage to the mean.\relax }}{57}{figure.caption.30}}
