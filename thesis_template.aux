\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{Permission to Use}{i}{section*.1}}
\@writefile{toc}{\contentsline {chapter}{Abstract}{iii}{section*.4}}
\@writefile{toc}{\contentsline {chapter}{Acknowledgements}{iv}{section*.6}}
\@writefile{toc}{\contentsline {chapter}{Contents}{vi}{section*.8}}
\@writefile{toc}{\contentsline {chapter}{List of Tables}{vii}{section*.9}}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{viii}{section*.10}}
\gdef \LT@i {\LT@entry 
    {1}{46.8612pt}\LT@entry 
    {1}{425.17003pt}}
\@writefile{toc}{\contentsline {chapter}{List of Abbreviations}{ix}{section*.11}}
\citation{Schwartz1977}
\citation{Courneya1992}
\citation{Nevill1999}
\citation{Agnew1994}
\citation{Unkelbach2010}
\citation{Forrest2005}
\citation{Dohmen2016}
\citation{Buraimo2010}
\citation{Lopez2018}
\citation{Benz2020}
\citation{Benz2020}
\citation{McHill2020}
\citation{nhl2020}
\citation{usatoday2020}
\citation{Unkelbach2010}
\citation{Buraimo2010}
\citation{Courneya1992}
\citation{Carron2005}
\citation{McHill2020}
\citation{Garicano2005}
\citation{Moskowitz2012}
\citation{McHill2020}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{Schwartz1977}
\citation{Courneya1992}
\citation{Carron2005}
\citation{Pollard2005a}
\citation{Gomez2011}
\citation{Benz2020}
\citation{McHill2020}
\citation{Benz2020}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{3}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Home Advantage}{3}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Related Work}{3}{subsection.2.1.1}}
\citation{Bradley1952}
\citation{Glickman1998}
\citation{Lopez2018}
\citation{Baio2010}
\citation{GlickmanText2017}
\citation{Benz2020}
\citation{Lopez2018}
\citation{GlickmanText2017}
\citation{Benz2020}
\citation{Baio2010}
\citation{Baio2010}
\citation{Lopez2018}
\citation{Pollard2005a}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Bayesian Inference}{5}{section.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Introduction}{5}{subsection.2.2.1}}
\newlabel{eq:orig_bayes_theorem}{{2.1}{5}{Introduction}{equation.2.2.1}{}}
\newlabel{eq:new_bayes_theorem}{{2.2}{6}{Introduction}{equation.2.2.2}{}}
\newlabel{eq:proportional_bayes_theorem}{{2.3}{6}{Introduction}{equation.2.2.3}{}}
\newlabel{eq:normalization_factor}{{2.4}{7}{Introduction}{equation.2.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Multilevel Modeling}{7}{subsection.2.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Markov Chain Monte Carlo}{10}{subsection.2.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Hamiltonian Monte Carlo}{13}{subsection.2.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Model Evaluation and Selection}{14}{subsection.2.2.5}}
\@writefile{toc}{\contentsline {subsubsection}{Cross Validation, Information Criterion, and Beyond}{14}{subsubsection*.13}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Example of how increasing model complexity leads to better model fit on the train-set, but can come at the cost of increasingly worse performance on the test-set. Model fit here is measured visually and in terms of mean-squared-error (MSE) and R squared ($R^2$). The dataset in (a) generated by a degree-2 polynomial with some added noise is split into train and test sets. A degree-1 polynomial underfits the data (b). A more complex degree-2 polynomial improves the fit (c). A far more complex degree-16 polynomial fits the train set extremely well but is overfit as evidenced by its poor test set performance (d).\relax }}{15}{figure.caption.14}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:overfitting_example}{{2.1}{15}{Example of how increasing model complexity leads to better model fit on the train-set, but can come at the cost of increasingly worse performance on the test-set. Model fit here is measured visually and in terms of mean-squared-error (MSE) and R squared ($R^2$). The dataset in (a) generated by a degree-2 polynomial with some added noise is split into train and test sets. A degree-1 polynomial underfits the data (b). A more complex degree-2 polynomial improves the fit (c). A far more complex degree-16 polynomial fits the train set extremely well but is overfit as evidenced by its poor test set performance (d).\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{15}{subfigure.1.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{15}{subfigure.1.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{15}{subfigure.1.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{15}{subfigure.1.4}}
\newlabel{eq:lppd}{{2.9}{17}{Cross Validation, Information Criterion, and Beyond}{equation.2.2.9}{}}
\newlabel{eq:waic}{{2.11}{18}{Cross Validation, Information Criterion, and Beyond}{equation.2.2.11}{}}
\newlabel{eq:psis-loo}{{2.12}{19}{Cross Validation, Information Criterion, and Beyond}{equation.2.2.12}{}}
\citation{Benz2020}
\citation{McHill2020}
\@writefile{toc}{\contentsline {subsubsection}{Posterior Predictive Checks}{20}{subsubsection*.15}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Contribution}{20}{section.2.3}}
\citation{Baio2010}
\citation{Glickman1998}
\citation{Lopez2018}
\citation{Benz2020}
\citation{Gelman2014}
\citation{Gelman2006}
\citation{McElreath2020}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methods}{22}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Multilevel Model}{22}{section.3.1}}
\citation{McElreath2020}
\citation{Gelman2014}
\citation{Gelman2006}
\citation{Baio2010}
\newlabel{eq:expected points}{{3.1}{23}{Multilevel Model}{equation.3.1.1}{}}
\newlabel{eq:sum to zero}{{3.2}{23}{Multilevel Model}{equation.3.1.2}{}}
\citation{pymc3}
\citation{Baio2010}
\citation{Benz2020}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Model Fit in PyMC3}{24}{subsection.3.1.1}}
\citation{Gelman1992}
\citation{Brooks1997}
\citation{Betancourt2017}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Negative Binomial}{25}{section.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Experiments}{25}{section.3.3}}
\citation{Benz2020}
\citation{Karlis2003}
\citation{Baio2010}
\citation{Benz2020}
\citation{Payne2018}
\citation{Cameron1990}
\citation{Vehtari2016}
\citation{Watanabe2010}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Results}{28}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Experiments}{28}{section.4.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Home Advantage}{28}{section.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Comparison of models via their Mean-Squared-Error (MSE) on train and test sets for each league. The complete-pooling model underfits, the no-pooling model overfits, and the partial-pooling model provides the best tradeoff in fitting the data while protecting against overfitting.\relax }}{29}{figure.caption.16}}
\newlabel{fig:mses}{{4.1}{29}{Comparison of models via their Mean-Squared-Error (MSE) on train and test sets for each league. The complete-pooling model underfits, the no-pooling model overfits, and the partial-pooling model provides the best tradeoff in fitting the data while protecting against overfitting.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {NHL}}}{29}{subfigure.1.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {NBA}}}{29}{subfigure.1.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {MLB}}}{29}{subfigure.1.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {NFL}}}{29}{subfigure.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Comparison of models via their estimated out of sample performance as measured by PSIS-LOO. Since the estimates are computed in the same way on the same data, their errors are highly correlated and the grey error bars represent a more accurate estimate of the standard error of the estimates relative to the best performing model.\relax }}{30}{figure.caption.17}}
\newlabel{fig:psis-loos}{{4.2}{30}{Comparison of models via their estimated out of sample performance as measured by PSIS-LOO. Since the estimates are computed in the same way on the same data, their errors are highly correlated and the grey error bars represent a more accurate estimate of the standard error of the estimates relative to the best performing model.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {NHL}}}{30}{subfigure.2.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {NBA}}}{30}{subfigure.2.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {MLB}}}{30}{subfigure.2.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {NFL}}}{30}{subfigure.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Distributions of the estimated home advantage for the NHL, NBA, MLB, and NFL for pre and post COVID adjusted seasons. Home advantage for playoffs are reported for NHL and NBA because that is when their COVID restricted games took place. Home advantage for regular season is reported for MLB and NFL as their respective playoff seasons are too small for stable results. Red distributions represent COVID-19 bubble adjusted seasons.\relax }}{31}{figure.caption.18}}
\newlabel{fig:ha_pooled}{{4.3}{31}{Distributions of the estimated home advantage for the NHL, NBA, MLB, and NFL for pre and post COVID adjusted seasons. Home advantage for playoffs are reported for NHL and NBA because that is when their COVID restricted games took place. Home advantage for regular season is reported for MLB and NFL as their respective playoff seasons are too small for stable results. Red distributions represent COVID-19 bubble adjusted seasons.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Distributions of the estimated home advantage for the NHL, NBA, MLB, and NFL over the past 5 seasons from 2016-2020. Home advantage for playoffs are reported for NHL and NBA because that is when their COVID restricted games took place. Home advantage for regular season is reported for MLB and NFL as their respective playoff seasons are too small for stable results. Red distributions represent COVID-19 bubble adjusted seasons.\relax }}{33}{figure.caption.19}}
\newlabel{fig:ha_main}{{4.4}{33}{Distributions of the estimated home advantage for the NHL, NBA, MLB, and NFL over the past 5 seasons from 2016-2020. Home advantage for playoffs are reported for NHL and NBA because that is when their COVID restricted games took place. Home advantage for regular season is reported for MLB and NFL as their respective playoff seasons are too small for stable results. Red distributions represent COVID-19 bubble adjusted seasons.\relax }{figure.caption.19}{}}
\citation{Karlis2003}
\citation{Baio2010}
\citation{Benz2020}
\citation{Payne2018}
\citation{Cameron1990}
\citation{Vehtari2016}
\citation{Watanabe2010}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Comparison of distribution of home points in the models and the observed data for each league. The Negative Binomial model noticeably provides a better overall fit across each league.\relax }}{35}{figure.caption.20}}
\newlabel{fig:comparisons}{{4.5}{35}{Comparison of distribution of home points in the models and the observed data for each league. The Negative Binomial model noticeably provides a better overall fit across each league.\relax }{figure.caption.20}{}}
\citation{McHill2020}
\citation{Hall2020}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion}{37}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{Pollard2005a}
\citation{pymc3}
\citation{Betancourt2017}
\citation{Lopez2018}
\citation{Glickman1998}
\citation{Karlis2003}
\citation{Baio2010}
\citation{Benz2020}
\citation{Gelman2006}
\citation{Gelman2014}
\citation{McElreath2020}
\citation{Benz2020}
\citation{Unkelbach2010}
\citation{Buraimo2010}
\citation{Dohmen2016}
\bibstyle{plain}
\bibdata{880Paper}
\@writefile{toc}{\contentsline {chapter}{References}{40}{section*.21}}
\@writefile{toc}{\contentsline {chapter}{Appendix \numberline {A}Sample Appendix}{40}{Appendix.1.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{Appendix \numberline {B}Another Sample Appendix}{41}{Appendix.1.B}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
